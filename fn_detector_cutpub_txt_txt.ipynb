{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU is available\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration Sections: ['DATASET', 'FEATURE_PROCESSING', 'EDA', 'PROCESS', 'MODELS']\n",
      "DropFeatures: 'title','subject','date', type: <class 'str'>\n",
      "LogisticRegression     : TRUE\n",
      "SVMGridSearch          : TRUE\n",
      "SVMRandomizedSearch    : TRUE\n",
      "KNNearestNeighbors     : FALSE\n",
      "DecisionTreeClassifier : TRUE\n",
      "DeepNeuralNetworks     : FALSE\n",
      "ConvolutionNetworks    : FALSE\n",
      "LanguagePatternsAlg    : FALSE\n",
      "BayesianOptimization   : FALSE\n",
      "Model Output File: FakeNewsModel_1728500216.499129.pkl\n",
      "grid_search_verbose: 3\n",
      "dataset_split: 0.35\n"
     ]
    }
   ],
   "source": [
    "import configparser\n",
    "import time\n",
    "\n",
    "current_time = str(time.time())\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read('FakeNewsModel_text_text.ini')\n",
    "print(f'Configuration Sections: {config.sections()}')\n",
    "\n",
    "model_rnn_bidirectional_fname  = config['DEFAULT']['model_rnn_bidirectional_fname']\n",
    "model_rnn_unidirectional_fname = config['DEFAULT']['model_rnn_unidirectional_fname']\n",
    "\n",
    "model_cnn_fname  = config['DEFAULT']['model_cnn_fname']\n",
    "model_randforest_fname = config['DEFAULT']['model_randforest_fname']\n",
    "model_adaboost_fname   = config['DEFAULT']['model_adaboost_fname']\n",
    "model_xgboost_fname    = config['DEFAULT']['model_xgboost_fname']\n",
    "model_bagging_fname    = config['DEFAULT']['model_bagging_fname']\n",
    "model_gradboost_fname  = config['DEFAULT']['model_gradboost_fname']\n",
    "model_svm_fname        = config['DEFAULT']['model_svm_fname']\n",
    "model_lgr_fname        = config['DEFAULT']['model_lgr_fname']\n",
    "\n",
    "\n",
    "DropFeatures = config['FEATURE_PROCESSING']['DropFeatures']\n",
    "print(f'DropFeatures: {DropFeatures}, type: {type(DropFeatures)}')\n",
    "\n",
    "Proc_LogisticRegression     = config['MODELS']['LogisticRegression'    ]\n",
    "Proc_SVMGridSearch          = config['MODELS']['SVMGridSearch'         ]\n",
    "Proc_SVMRandomizedSearch    = config['MODELS']['SVMRandomizedSearch'   ]\n",
    "Proc_KNNearestNeighbors     = config['MODELS']['KNNearestNeighbors'    ]\n",
    "Proc_DecisionTreeClassifier = config['MODELS']['DecisionTreeClassifier']\n",
    "Proc_DeepNeuralNetworks     = config['MODELS']['DeepNeuralNetworks'    ]\n",
    "Proc_ConvolutionalNetworks  = config['MODELS']['ConvolutionalNetworks' ]\n",
    "Proc_LanguagePatternsAlg    = config['MODELS']['LanguagePatternsAlg'   ]\n",
    "Proc_BayesianOptimization   = config['MODELS']['BayesianOptimization'  ]\n",
    "\n",
    "print(f'LogisticRegression     : {Proc_LogisticRegression    }')\n",
    "print(f'SVMGridSearch          : {Proc_SVMGridSearch         }')\n",
    "print(f'SVMRandomizedSearch    : {Proc_SVMRandomizedSearch   }')\n",
    "print(f'KNNearestNeighbors     : {Proc_KNNearestNeighbors    }')\n",
    "print(f'DecisionTreeClassifier : {Proc_DecisionTreeClassifier}')\n",
    "print(f'DeepNeuralNetworks     : {Proc_DeepNeuralNetworks    }')\n",
    "print(f'ConvolutionNetworks    : {Proc_ConvolutionalNetworks }')\n",
    "print(f'LanguagePatternsAlg    : {Proc_LanguagePatternsAlg   }')\n",
    "print(f'BayesianOptimization   : {Proc_BayesianOptimization  }')\n",
    "\n",
    "model_outFile = config['DEFAULT']['model_outputFile'] + current_time + '.pkl'\n",
    "model_inFile  = config['DEFAULT']['model_inputFile' ] + current_time + '.pkl'\n",
    "model_prefix_    = config['DEFAULT']['model_prefix'    ]\n",
    "print(f'Model Output File: {model_outFile}')\n",
    "\n",
    "grid_search_verbose = int(config['PROCESS']['gridSearchVerbose'])\n",
    "print(f'grid_search_verbose: {grid_search_verbose}')\n",
    "\n",
    "dataset_split = float(config['DEFAULT']['train_test_split'])\n",
    "print(f'dataset_split: {dataset_split}')\n",
    "\n",
    "file_true = config['DATASET']['TrueFile']\n",
    "file_fake = config['DATASET']['FakeFile']\n",
    "\n",
    "nn_models_epochs          = 20\n",
    "nn_models_batchsize       = 128\n",
    "nn_models_validationSplit = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_fake input: (23481, 10)\n",
      "data_true input: (21417, 10)\n"
     ]
    }
   ],
   "source": [
    "# Import datasets, and setup classification class\n",
    "from tensorflow.keras.layers import Embedding, LSTM, GRU, Dense, Dropout, Bidirectional\n",
    "dataset_fraction = 1.0\n",
    "\n",
    "data_fake = pd.read_csv( file_fake ).sample(frac=dataset_fraction)\n",
    "data_true = pd.read_csv( file_true ).sample(frac=dataset_fraction)\n",
    "\n",
    "print(f'data_fake input: {data_fake.shape}')\n",
    "print(f'data_true input: {data_true.shape}')\n",
    "\n",
    "data_fake['class'] = 0\n",
    "data_true['class'] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom function to split text, remove publisher information from article text\n",
    "def split_text(text):\n",
    "    if '-' in text[:30]:\n",
    "        parts = text.split('-', 1)\n",
    "        return parts[0], parts[1]\n",
    "    else:\n",
    "        return 'None', text\n",
    "    \n",
    "data_fake[['publisher', 'text']] = data_fake['text'].apply(lambda x: pd.Series(split_text(x)))\n",
    "data_true[['publisher', 'text']] = data_true['text'].apply(lambda x: pd.Series(split_text(x)))\n",
    "\n",
    "num_fake = len(data_fake)\n",
    "num_true = len(data_true)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New DataFrame shape: (42834, 12)\n"
     ]
    }
   ],
   "source": [
    "# Find the minimum number of records\n",
    "min_records = min(num_fake, num_true)\n",
    "\n",
    "# Determine which DataFrame has more records\n",
    "if num_fake > num_true:\n",
    "    data_fake_sampled = data_fake.sample(n=min_records)\n",
    "    data = pd.concat([data_true, data_fake_sampled], ignore_index=True)\n",
    "    \n",
    "else:\n",
    "    data_true_sampled = data_true.sample(n=min_records)\n",
    "    data = pd.concat([data_fake, data_true_sampled], ignore_index=True)\n",
    "\n",
    "# Print the shape of the new DataFrame\n",
    "print(f'New DataFrame shape: {data.shape}')\n",
    "data = data.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "data = data.drop(columns=['title','subject','date','sentiment','emotion','objectivity','intent','assertions','publisher'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text filters for cleanup text passages\n",
    "def wordopt( text ):\n",
    "    text = text.lower()\n",
    "    text = re.sub('\\[.*?\\]','',text)\n",
    "    text = re.sub('\\\\W', ' ',  text)\n",
    "    text = re.sub('https?://\\S+|www\\.\\S+','',text)\n",
    "    text = re.sub('<.*?>+','',text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation),'',text)\n",
    "    text = re.sub('\\n','',text)\n",
    "    text = re.sub('\\w*\\d\\w*','',text)\n",
    "    return text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################################\n",
    "#  Data Preparation                                                      #\n",
    "#  Prepare the data frame in formats for each type of model used         #\n",
    "#  a) Split dataset into Train / Test                                    #\n",
    "#  b) TDIDF vectorization transformation of Training and Testing data    #\n",
    "#  c) Transform Training / Testing Data into Dense Arrays                #\n",
    "#  d) Reshape dense data arrays for input to RNN models                  #\n",
    "##########################################################################\n",
    "# Remove stop words, and escape characters\n",
    "data['text'] = data['text'].apply(wordopt)\n",
    "x = data[['text', 'summary']]\n",
    "y = data['class']\n",
    "\n",
    "Xv_0_train, Xv_0_test, y_train, y_test = train_test_split(x,y,test_size=dataset_split)\n",
    "Xv_train = Xv_0_train['text']\n",
    "Xv_test  = Xv_0_test['text']\n",
    "\n",
    "\n",
    "max_features = 5000\n",
    "vectorization = TfidfVectorizer( max_features=max_features )\n",
    "X_train = vectorization.fit_transform( Xv_train )\n",
    "X_train_dense = X_train.toarray()\n",
    "\n",
    "X_test  = vectorization.transform( Xv_test )\n",
    "X_test_dense = X_test.toarray()\n",
    "\n",
    "y_train = np.array(y_train)\n",
    "y_test  = np.array(y_test )\n",
    "\n",
    "X_train_reshaped = X_train_dense.reshape((X_train_dense.shape[0], 1, X_train_dense.shape[1]))  # Shape: (num_samples, 1, num_features)\n",
    "X_test_reshaped  = X_test_dense.reshape((X_test_dense.shape[0],   1, X_test_dense.shape[1]))   # Shape: (num_samples, 1, num_features)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Dropout, Bidirectional, LSTM, GRU, Dense\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "max_words = 30000           # Maximum number of words to consider in vocabulary\n",
    "max_len   = 2000            # Maximum length of sequences (padding/truncating)\n",
    "\n",
    "embedding_dim = 128         # Dimension of word embedding\n",
    "\n",
    "model_results = []\n",
    "model_results_headers = ['Model','Training Time','Testing Time','Accuracy','Precision(weighted)','Recall(weighted)','f1-score(weighted)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN Models\n",
    "#   a) BiLSTM, BiGRU - BiDirectional  64-LSTM, 32-GRU hidden layers\n",
    "#   b) LSTM, GRU     - UniDirectional 64-LSTM, 32-GRU hidden layers\n",
    "\n",
    "from keras.models import load_model, Sequential, Model\n",
    "from keras.layers import Dense, Conv1D, MaxPooling1D, LSTM, Dropout, Flatten, Input, Embedding\n",
    "from keras.layers import Bidirectional\n",
    "\n",
    "def create_model_Rnn_BiLSTM_BiGRU( modelType, max_words=max_words, embedding_dim=embedding_dim ):\n",
    "    print(f'Creating model type: {modelType}')\n",
    "    model = Sequential([\n",
    "        Dropout(0.2),\n",
    "        Bidirectional(LSTM(64, return_sequences=True)),\n",
    "        Dropout(0.2),\n",
    "        Bidirectional(GRU(32)),\n",
    "        Dropout(0.2),\n",
    "        Dense(4, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(1, activation='sigmoid')  # Sigmoid for binary classification\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def create_model_Rnn_LSTM_GRU( modelType, max_words=max_words, embedding_dim=embedding_dim ):\n",
    "    print(f'Creating model type: {modelType}')\n",
    "    model = Sequential([\n",
    "        Dropout(0.2),\n",
    "        LSTM(64, return_sequences=True),\n",
    "        Dropout(0.2),\n",
    "        GRU(32),\n",
    "        Dropout(0.2),\n",
    "        Dense(4, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# CNN Models\n",
    "#   a) CNN + RNN + Transformer\n",
    "\n",
    "def create_model_CnnRnnTransformer(input_shape):\n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    # CNN part\n",
    "    cnn_layer = Conv1D(filters=128, kernel_size=5, activation='relu')(inputs)\n",
    "    cnn_layer = MaxPooling1D(pool_size=2)(cnn_layer)\n",
    "    cnn_layer = Conv1D(filters=64, kernel_size=5, activation='relu')(cnn_layer)\n",
    "    cnn_layer = MaxPooling1D(pool_size=2)(cnn_layer)\n",
    "\n",
    "    # RNN (LSTM or GRU)\n",
    "    rnn_layer = LSTM(128, return_sequences=True)(cnn_layer)\n",
    "    rnn_layer = Dropout(0.2)(rnn_layer)\n",
    "\n",
    "    # Optionally, use Bidirectional LSTM/GRU for better performance\n",
    "    rnn_layer = Bidirectional(LSTM(128,return_sequences=True))(rnn_layer)\n",
    "\n",
    "    # Transformer Layer using MultiHeadAttention\n",
    "    transformer_layer = tf.keras.layers.MultiHeadAttention(num_heads=4, key_dim=128)(rnn_layer, rnn_layer)\n",
    "    \n",
    "    # Pooling to reduce the sequence to a fixed size (optional global average pooling)\n",
    "    pooled_output = tf.keras.layers.GlobalAveragePooling1D()(transformer_layer)\n",
    "\n",
    "    # Fully Connected Layer\n",
    "    dense_layer = Dense(64, activation='relu')(pooled_output)\n",
    "    dense_layer = Dropout(0.5)(dense_layer)\n",
    "\n",
    "    # Output Layer (assuming binary classification)\n",
    "    outputs = Dense(1, activation='sigmoid')(dense_layer)\n",
    "\n",
    "    # Define the model\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: rnn_bidirectional_detector.h5 loaded ...\n",
      "Epoch 1/20\n",
      "175/175 [==============================] - 7s 15ms/step - loss: 0.3862 - accuracy: 0.8918 - val_loss: 0.1025 - val_accuracy: 0.9627\n",
      "Epoch 2/20\n",
      "175/175 [==============================] - 2s 10ms/step - loss: 0.1174 - accuracy: 0.9605 - val_loss: 0.0626 - val_accuracy: 0.9793\n",
      "Epoch 3/20\n",
      "175/175 [==============================] - 2s 9ms/step - loss: 0.0769 - accuracy: 0.9760 - val_loss: 0.0551 - val_accuracy: 0.9840\n",
      "Epoch 4/20\n",
      "175/175 [==============================] - 2s 10ms/step - loss: 0.0607 - accuracy: 0.9811 - val_loss: 0.0518 - val_accuracy: 0.9856\n",
      "Epoch 5/20\n",
      "175/175 [==============================] - 2s 10ms/step - loss: 0.0538 - accuracy: 0.9837 - val_loss: 0.0540 - val_accuracy: 0.9849\n",
      "Epoch 6/20\n",
      "175/175 [==============================] - 2s 10ms/step - loss: 0.0465 - accuracy: 0.9855 - val_loss: 0.0583 - val_accuracy: 0.9831\n",
      "Epoch 7/20\n",
      "175/175 [==============================] - 2s 10ms/step - loss: 0.0452 - accuracy: 0.9864 - val_loss: 0.0626 - val_accuracy: 0.9835\n",
      "Epoch 8/20\n",
      "175/175 [==============================] - 2s 11ms/step - loss: 0.0431 - accuracy: 0.9873 - val_loss: 0.0578 - val_accuracy: 0.9840\n",
      "Epoch 9/20\n",
      "175/175 [==============================] - 2s 11ms/step - loss: 0.0378 - accuracy: 0.9888 - val_loss: 0.0622 - val_accuracy: 0.9837\n",
      "Epoch 10/20\n",
      "175/175 [==============================] - 2s 11ms/step - loss: 0.0399 - accuracy: 0.9886 - val_loss: 0.0560 - val_accuracy: 0.9847\n",
      "Epoch 11/20\n",
      "175/175 [==============================] - 2s 12ms/step - loss: 0.0348 - accuracy: 0.9903 - val_loss: 0.0571 - val_accuracy: 0.9844\n",
      "Epoch 12/20\n",
      "175/175 [==============================] - 2s 12ms/step - loss: 0.0356 - accuracy: 0.9889 - val_loss: 0.0600 - val_accuracy: 0.9842\n",
      "Epoch 13/20\n",
      "175/175 [==============================] - 2s 10ms/step - loss: 0.0321 - accuracy: 0.9909 - val_loss: 0.0581 - val_accuracy: 0.9862\n",
      "Epoch 14/20\n",
      "175/175 [==============================] - 2s 14ms/step - loss: 0.0340 - accuracy: 0.9901 - val_loss: 0.0633 - val_accuracy: 0.9849\n",
      "Epoch 15/20\n",
      "175/175 [==============================] - 2s 12ms/step - loss: 0.0335 - accuracy: 0.9910 - val_loss: 0.0648 - val_accuracy: 0.9844\n",
      "Epoch 16/20\n",
      "175/175 [==============================] - 2s 10ms/step - loss: 0.0308 - accuracy: 0.9914 - val_loss: 0.0640 - val_accuracy: 0.9846\n",
      "Epoch 17/20\n",
      "175/175 [==============================] - 2s 12ms/step - loss: 0.0263 - accuracy: 0.9930 - val_loss: 0.0718 - val_accuracy: 0.9833\n",
      "Epoch 18/20\n",
      "175/175 [==============================] - 2s 12ms/step - loss: 0.0281 - accuracy: 0.9925 - val_loss: 0.0684 - val_accuracy: 0.9842\n",
      "Epoch 19/20\n",
      "175/175 [==============================] - 2s 11ms/step - loss: 0.0278 - accuracy: 0.9924 - val_loss: 0.0784 - val_accuracy: 0.9837\n",
      "Epoch 20/20\n",
      "175/175 [==============================] - 2s 11ms/step - loss: 0.0284 - accuracy: 0.9918 - val_loss: 0.0738 - val_accuracy: 0.9842\n",
      "\n",
      "Training Time: 43.38 seconds\n",
      "\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0758 - accuracy: 0.9844\n",
      "Test Accuracy: 0.9844\n",
      "469/469 [==============================] - 2s 3ms/step\n",
      "rnn_bidirectional_detector.h5: Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98      7465\n",
      "           1       0.99      0.98      0.98      7527\n",
      "\n",
      "    accuracy                           0.98     14992\n",
      "   macro avg       0.98      0.98      0.98     14992\n",
      "weighted avg       0.98      0.98      0.98     14992\n",
      "\n",
      "rnn_bidirectional_detector.h5: Accuracy Score: 0.9844\n",
      "model: rnn_unidirectional_detector.h5 loaded ...\n",
      "Epoch 1/20\n",
      "175/175 [==============================] - 3s 10ms/step - loss: 0.3374 - accuracy: 0.8949 - val_loss: 0.1069 - val_accuracy: 0.9587\n",
      "Epoch 2/20\n",
      "175/175 [==============================] - 1s 7ms/step - loss: 0.1377 - accuracy: 0.9514 - val_loss: 0.0711 - val_accuracy: 0.9752\n",
      "Epoch 3/20\n",
      "175/175 [==============================] - 1s 7ms/step - loss: 0.0954 - accuracy: 0.9700 - val_loss: 0.0581 - val_accuracy: 0.9801\n",
      "Epoch 4/20\n",
      "175/175 [==============================] - 1s 7ms/step - loss: 0.0737 - accuracy: 0.9768 - val_loss: 0.0529 - val_accuracy: 0.9826\n",
      "Epoch 5/20\n",
      "175/175 [==============================] - 1s 7ms/step - loss: 0.0600 - accuracy: 0.9813 - val_loss: 0.0538 - val_accuracy: 0.9847\n",
      "Epoch 6/20\n",
      "175/175 [==============================] - 1s 7ms/step - loss: 0.0577 - accuracy: 0.9823 - val_loss: 0.0554 - val_accuracy: 0.9846\n",
      "Epoch 7/20\n",
      "175/175 [==============================] - 1s 8ms/step - loss: 0.0508 - accuracy: 0.9850 - val_loss: 0.0567 - val_accuracy: 0.9844\n",
      "Epoch 8/20\n",
      "175/175 [==============================] - 1s 7ms/step - loss: 0.0455 - accuracy: 0.9870 - val_loss: 0.0579 - val_accuracy: 0.9840\n",
      "Epoch 9/20\n",
      "175/175 [==============================] - 1s 8ms/step - loss: 0.0474 - accuracy: 0.9854 - val_loss: 0.0551 - val_accuracy: 0.9842\n",
      "Epoch 10/20\n",
      "175/175 [==============================] - 1s 7ms/step - loss: 0.0426 - accuracy: 0.9878 - val_loss: 0.0546 - val_accuracy: 0.9838\n",
      "Epoch 11/20\n",
      "175/175 [==============================] - 1s 7ms/step - loss: 0.0394 - accuracy: 0.9894 - val_loss: 0.0547 - val_accuracy: 0.9829\n",
      "Epoch 12/20\n",
      "175/175 [==============================] - 1s 7ms/step - loss: 0.0386 - accuracy: 0.9887 - val_loss: 0.0615 - val_accuracy: 0.9851\n",
      "Epoch 13/20\n",
      "175/175 [==============================] - 1s 7ms/step - loss: 0.0344 - accuracy: 0.9908 - val_loss: 0.0616 - val_accuracy: 0.9849\n",
      "Epoch 14/20\n",
      "175/175 [==============================] - 1s 8ms/step - loss: 0.0370 - accuracy: 0.9886 - val_loss: 0.0687 - val_accuracy: 0.9837\n",
      "Epoch 15/20\n",
      "175/175 [==============================] - 1s 7ms/step - loss: 0.0393 - accuracy: 0.9889 - val_loss: 0.0617 - val_accuracy: 0.9855\n",
      "Epoch 16/20\n",
      "175/175 [==============================] - 1s 7ms/step - loss: 0.0330 - accuracy: 0.9907 - val_loss: 0.0665 - val_accuracy: 0.9849\n",
      "Epoch 17/20\n",
      "175/175 [==============================] - 1s 8ms/step - loss: 0.0316 - accuracy: 0.9909 - val_loss: 0.0643 - val_accuracy: 0.9851\n",
      "Epoch 18/20\n",
      "175/175 [==============================] - 1s 7ms/step - loss: 0.0327 - accuracy: 0.9909 - val_loss: 0.0723 - val_accuracy: 0.9840\n",
      "Epoch 19/20\n",
      "175/175 [==============================] - 1s 7ms/step - loss: 0.0305 - accuracy: 0.9914 - val_loss: 0.0679 - val_accuracy: 0.9855\n",
      "Epoch 20/20\n",
      "175/175 [==============================] - 1s 7ms/step - loss: 0.0322 - accuracy: 0.9914 - val_loss: 0.0649 - val_accuracy: 0.9864\n",
      "\n",
      "Training Time: 26.74 seconds\n",
      "\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.0744 - accuracy: 0.9834\n",
      "Test Accuracy: 0.9834\n",
      "469/469 [==============================] - 1s 2ms/step\n",
      "rnn_unidirectional_detector.h5: Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98      7465\n",
      "           1       0.98      0.98      0.98      7527\n",
      "\n",
      "    accuracy                           0.98     14992\n",
      "   macro avg       0.98      0.98      0.98     14992\n",
      "weighted avg       0.98      0.98      0.98     14992\n",
      "\n",
      "rnn_unidirectional_detector.h5: Accuracy Score: 0.9834\n"
     ]
    }
   ],
   "source": [
    "############################################################################################\n",
    "#  Executing Neural Network Models - a) RNN + BiLSTM  + BiGRU                              #\n",
    "#                                    b) RNN + UniLSTM + UniGRU                             #\n",
    "#  epcochs   : 20                                                                          #\n",
    "#  batch_size: 128                                                                         #\n",
    "############################################################################################\n",
    "\n",
    "import os\n",
    "import joblib\n",
    "from keras.models import load_model\n",
    "\n",
    "models = { model_rnn_bidirectional_fname: create_model_Rnn_BiLSTM_BiGRU, model_rnn_unidirectional_fname: create_model_Rnn_LSTM_GRU }\n",
    "\n",
    "for model_name, model_function in models.items():\n",
    "    if( os.path.exists( model_name ) ):\n",
    "        model = load_model( model_name )\n",
    "        print(f'model: {model_name} loaded ...')\n",
    "    else:\n",
    "        model = model_function(\"RNN_BiLSTM_BiGRU\", max_words=max_words, embedding_dim=embedding_dim)\n",
    "\n",
    "    start_time = time.time()\n",
    "    history = model.fit(X_train_reshaped, y_train, \n",
    "                        epochs=nn_models_epochs, \n",
    "                        batch_size=nn_models_batchsize, \n",
    "                        validation_split=nn_models_validationSplit, \n",
    "                        verbose=1)\n",
    "    train_time = time.time() - start_time\n",
    "    print(f'\\nTraining Time: {train_time:.2f} seconds\\n')\n",
    "    start_time = time.time()\n",
    "    test_loss, test_accuracy = model.evaluate(X_test_reshaped, y_test)\n",
    "    \n",
    "    test_time = time.time() - start_time\n",
    "    print(f'Test Accuracy: {test_accuracy:.4f}')\n",
    "\n",
    "    y_pred = (model.predict(X_test_reshaped) > 0.50).astype(\"int32\")\n",
    "    print(f\"{model_name}: Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "    print(f\"{model_name}: Accuracy Score: {accuracy_score(y_test, y_pred):.4f}\")    \n",
    "    \n",
    "    report = classification_report(y_test,y_pred,output_dict=True)\n",
    "    precision = report['weighted avg']['precision']\n",
    "    recall    = report['weighted avg']['recall']\n",
    "    f1_score  = report['weighted avg']['f1-score']\n",
    "\n",
    "    model_results.append([model_name, f'{train_time:.2f} sec', f'{test_time:.2f} sec', f'{test_accuracy:.4f}',f'{precision:.4f}',f'{recall:.4f}',f'{f1_score:.4f}'])\n",
    "    model.save(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_name: randforest_detector.h5\n",
      "model_name: adaboost_detector.h5\n",
      "model_name: gradboost_detector.h5\n",
      "#####  randforest_detector.h5  #####\n",
      "model: randforest_detector.h5 loaded ...\n",
      "regular training fit\n",
      "Training Time: 5.11 seconds\n",
      "\n",
      "[[7280  185]\n",
      " [ 442 7085]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.98      0.96      7465\n",
      "           1       0.97      0.94      0.96      7527\n",
      "\n",
      "    accuracy                           0.96     14992\n",
      "   macro avg       0.96      0.96      0.96     14992\n",
      "weighted avg       0.96      0.96      0.96     14992\n",
      "\n",
      "#####  adaboost_detector.h5  #####\n",
      "model: adaboost_detector.h5 loaded ...\n",
      "regular training fit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fkao9\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Time: 26.79 seconds\n",
      "\n",
      "[[6830  635]\n",
      " [ 487 7040]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.91      0.92      7465\n",
      "           1       0.92      0.94      0.93      7527\n",
      "\n",
      "    accuracy                           0.93     14992\n",
      "   macro avg       0.93      0.93      0.93     14992\n",
      "weighted avg       0.93      0.93      0.93     14992\n",
      "\n",
      "#####  gradboost_detector.h5  #####\n",
      "model: gradboost_detector.h5 loaded ...\n",
      "regular training fit\n",
      "Training Time: 65.83 seconds\n",
      "\n",
      "[[6730  735]\n",
      " [ 849 6678]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.90      0.89      7465\n",
      "           1       0.90      0.89      0.89      7527\n",
      "\n",
      "    accuracy                           0.89     14992\n",
      "   macro avg       0.89      0.89      0.89     14992\n",
      "weighted avg       0.89      0.89      0.89     14992\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "models_cpu = {\n",
    "    model_randforest_fname : RandomForestClassifier,\n",
    "    model_adaboost_fname   : AdaBoostClassifier,\n",
    "#    model_bagging_fname    : BaggingClassifier,\n",
    "    model_gradboost_fname  : GradientBoostingClassifier\n",
    "}\n",
    "for model_name, model_ in models_cpu.items():\n",
    "    print(f'model_name: {model_name}')\n",
    "\n",
    "model_n_estimators=10\n",
    "for model_name, model_function in models_cpu.items():\n",
    "    print(f'#####  {model_name}  #####')\n",
    "\n",
    "    if( os.path.exists( model_name ) ):\n",
    "        model = joblib.load( model_name )\n",
    "        print(f'model: {model_name} loaded ...')\n",
    "    else:\n",
    "        model = model_function(n_estimators=model_n_estimators)\n",
    "    \n",
    "    # Check if the model supports partial fitting\n",
    "    start_time = time.time()\n",
    "    if hasattr(model, 'partial_fit'):\n",
    "        # For partial fit, we need to specify classes for classification models\n",
    "        print(f'partial training fit')\n",
    "        classes = np.unique(y_train)\n",
    "        model.partial_fit(X_train_dense, y_train, classes=classes)\n",
    "    else:\n",
    "        print(f'regular training fit')\n",
    "        model.fit(X_train_dense, y_train)      \n",
    "    \n",
    "    train_time = time.time() - start_time\n",
    "    print(f'Training Time: {train_time:.2f} seconds\\n')\n",
    "    start_time = time.time()\n",
    "    y_pred = model.predict(X_test_dense)\n",
    "    test_time = time.time() - start_time\n",
    "    \n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    print(f'{conf_matrix}')\n",
    "    print(f'{classification_report(y_test, y_pred)}')\n",
    "\n",
    "    test_accuracy = accuracy_score(y_test,y_pred)\n",
    "    \n",
    "    report = classification_report(y_test,y_pred,output_dict=True)\n",
    "    precision = report['weighted avg']['precision']\n",
    "    recall    = report['weighted avg']['recall']\n",
    "    f1_score  = report['weighted avg']['f1-score']\n",
    "\n",
    "    model_results.append([model_name, f'{train_time:.2f} sec', f'{test_time:.2f} sec', f'{test_accuracy:.4f}',f'{precision:.4f}',f'{recall:.4f}',f'{f1_score:.4f}'])\n",
    "   \n",
    "    joblib.dump( model, model_name )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fkao9\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [12:00:21] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"predictor\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Time: 2.46 seconds\n",
      "\n",
      "Confusion Matrix for XGBoost (GPU):\n",
      "[[7305  160]\n",
      " [ 115 7412]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98      7465\n",
      "           1       0.98      0.98      0.98      7527\n",
      "\n",
      "    accuracy                           0.98     14992\n",
      "   macro avg       0.98      0.98      0.98     14992\n",
      "weighted avg       0.98      0.98      0.98     14992\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['xgboost_detector.h5']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##### XgBoost - CUDA #####\n",
    "import xgboost as xgb\n",
    "import os\n",
    "\n",
    "model_name = model_xgboost_fname\n",
    "\n",
    "dtrain = xgb.DMatrix(X_train_dense, label=y_train)\n",
    "dtest  = xgb.DMatrix(X_test_dense,  label=y_test )\n",
    "\n",
    "start_time = time.time()\n",
    "params = {\n",
    "    'objective': 'binary:logistic',  # Assuming binary classification\n",
    "    'tree_method': 'hist',           # Use GPU for training\n",
    "    'device'     : 'cuda',           # Use CUDA for GPU\n",
    "    'max_depth'  : 4,\n",
    "    'predictor'  : 'cpu_predictor'   # GPU for prediction as well\n",
    "}\n",
    "\n",
    "model = xgb.train(params, dtrain, num_boost_round=50)\n",
    "train_time = time.time() - start_time\n",
    "print(f'Training Time: {train_time:.2f} seconds\\n')\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "y_pred = model.predict(dtest)\n",
    "y_pred = [1 if pred > 0.5 else 0 for pred in y_pred]  # Convert probabilities to class labels\n",
    "\n",
    "test_time = time.time() - start_time\n",
    "\n",
    "# Confusion matrix and classification report\n",
    "print(\"Confusion Matrix for XGBoost (GPU):\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "test_accuracy = accuracy_score(y_test,y_pred)\n",
    "\n",
    "report = classification_report(y_test,y_pred,output_dict=True)\n",
    "precision = report['weighted avg']['precision']\n",
    "recall    = report['weighted avg']['recall']\n",
    "f1_score  = report['weighted avg']['f1-score']\n",
    "\n",
    "model_results.append([model_name, f'{train_time:.2f} sec', f'{test_time:.2f} sec', f'{test_accuracy:.4f}',f'{precision:.4f}',f'{recall:.4f}',f'{f1_score:.4f}'])\n",
    "\n",
    "joblib.dump( model,model_name )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ModelClassification_GridSearch(classifier, classifier_text, X_train,y_train,X_test,y_test,param_grid):\n",
    "    start_time = time.time()\n",
    "\n",
    "    grid_search = GridSearchCV(classifier, param_grid, cv=10, n_jobs=-1, scoring='accuracy',verbose=grid_search_verbose)\n",
    "    model_fit_param = grid_search.fit(X_train, y_train)\n",
    "    best_regressor  = grid_search.best_estimator_\n",
    "\n",
    "    train_time = time.time() - start_time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    y_pred                 = best_regressor.predict(X_test)\n",
    "    \n",
    "    test_time = time.time() - start_time\n",
    "    \n",
    "    model_accuracy         = accuracy_score(y_test,y_pred)\n",
    "    model_confusion_matrix = confusion_matrix(y_test,y_pred)\n",
    "    \n",
    "    report_classification = classification_report(y_test,y_pred, output_dict=True)\n",
    "\n",
    "    grid_training_score = best_regressor.score(X_train,y_train)\n",
    "    grid_testing_score  = best_regressor.score(X_test, y_test )\n",
    "\n",
    "    precision = report_classification['weighted avg']['precision']\n",
    "    recall    = report_classification['weighted avg']['recall']\n",
    "    f1_score  = report_classification['weighted avg']['f1-score']\n",
    "\n",
    "    model_results.append([classifier_text, f'{train_time:.2f} sec', f'{test_time:.2f} sec', f'{test_accuracy:.4f}',f'{precision:.4f}',f'{recall:.4f}',f'{f1_score:.4f}'])\n",
    "    \n",
    "    joblib.dump( best_regressor, classifier_text )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 9 candidates, totalling 90 fits\n",
      "Fitting 10 folds for each of 6 candidates, totalling 60 fits\n"
     ]
    }
   ],
   "source": [
    "if Proc_SVMGridSearch == 'TRUE':\n",
    "    param_grid_svc = {\n",
    "        'C': [0.1,1, 10], \n",
    "        'gamma': [1,0.1,0.01]\n",
    "    } \n",
    "    classifier_gridsearch_report = ModelClassification_GridSearch(SVC(), 'SVMGridSearch', X_train, y_train, X_test, y_test, param_grid_svc) \n",
    "\n",
    "if Proc_LogisticRegression == 'TRUE':\n",
    "    param_grid_lr = {\n",
    "        'C': [0.1, 1, 10],\n",
    "        'solver': ['liblinear', 'saga']\n",
    "    }\n",
    "    classifier_gridsearch_report = ModelClassification_GridSearch(LogisticRegression(), 'Logistic Regression GridSearch', X_train, y_train, X_test, y_test, param_grid_lr)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+---------------+--------------+----------+---------------------+------------------+--------------------+\n",
      "|             Model              | Training Time | Testing Time | Accuracy | Precision(weighted) | Recall(weighted) | f1-score(weighted) |\n",
      "+--------------------------------+---------------+--------------+----------+---------------------+------------------+--------------------+\n",
      "| rnn_bidirectional_detector.h5  |   43.38 sec   |   2.29 sec   |  0.9844  |       0.9844        |      0.9844      |       0.9844       |\n",
      "| rnn_unidirectional_detector.h5 |   26.74 sec   |   1.61 sec   |  0.9834  |       0.9834        |      0.9834      |       0.9834       |\n",
      "|     randforest_detector.h5     |   5.11 sec    |   0.20 sec   |  0.9582  |       0.9587        |      0.9582      |       0.9582       |\n",
      "|      adaboost_detector.h5      |   26.79 sec   |   1.28 sec   |  0.9252  |       0.9253        |      0.9252      |       0.9251       |\n",
      "|     gradboost_detector.h5      |   65.83 sec   |   0.18 sec   |  0.8943  |       0.8944        |      0.8943      |       0.8943       |\n",
      "|      xgboost_detector.h5       |   2.46 sec    |   0.10 sec   |  0.9817  |       0.9817        |      0.9817      |       0.9817       |\n",
      "|         SVMGridSearch          |  5001.85 sec  |  109.88 sec  |  0.9817  |       0.9884        |      0.9884      |       0.9884       |\n",
      "| Logistic Regression GridSearch |   13.67 sec   |   0.01 sec   |  0.9817  |       0.9857        |      0.9857      |       0.9857       |\n",
      "+--------------------------------+---------------+--------------+----------+---------------------+------------------+--------------------+\n"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "print( tabulate(model_results, model_results_headers, tablefmt='pretty') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
