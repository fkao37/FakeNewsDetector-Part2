{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU is available\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration Sections: ['DATASET', 'FEATURE_PROCESSING', 'EDA', 'PROCESS', 'MODELS']\n",
      "DropFeatures: 'title','subject','date', type: <class 'str'>\n",
      "LogisticRegression     : TRUE\n",
      "SVMGridSearch          : TRUE\n",
      "SVMRandomizedSearch    : TRUE\n",
      "KNNearestNeighbors     : FALSE\n",
      "DecisionTreeClassifier : TRUE\n",
      "DeepNeuralNetworks     : FALSE\n",
      "ConvolutionNetworks    : FALSE\n",
      "LanguagePatternsAlg    : FALSE\n",
      "BayesianOptimization   : FALSE\n",
      "Model Output File: FakeNewsModel_1728529747.687975.pkl\n",
      "grid_search_verbose: 3\n",
      "dataset_split: 0.35\n"
     ]
    }
   ],
   "source": [
    "import configparser\n",
    "import time\n",
    "\n",
    "current_time = str(time.time())\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read('FakeNewsModel_sum_txt.ini')\n",
    "print(f'Configuration Sections: {config.sections()}')\n",
    "\n",
    "model_rnn_bidirectional_fname  = config['DEFAULT']['model_rnn_bidirectional_fname']\n",
    "model_rnn_unidirectional_fname = config['DEFAULT']['model_rnn_unidirectional_fname']\n",
    "\n",
    "model_cnn_fname  = config['DEFAULT']['model_cnn_fname']\n",
    "model_randforest_fname = config['DEFAULT']['model_randforest_fname']\n",
    "model_adaboost_fname   = config['DEFAULT']['model_adaboost_fname']\n",
    "model_xgboost_fname    = config['DEFAULT']['model_xgboost_fname']\n",
    "model_bagging_fname    = config['DEFAULT']['model_bagging_fname']\n",
    "model_gradboost_fname  = config['DEFAULT']['model_gradboost_fname']\n",
    "model_svm_fname        = config['DEFAULT']['model_svm_fname']\n",
    "model_lgr_fname        = config['DEFAULT']['model_lgr_fname']\n",
    "\n",
    "\n",
    "DropFeatures = config['FEATURE_PROCESSING']['DropFeatures']\n",
    "print(f'DropFeatures: {DropFeatures}, type: {type(DropFeatures)}')\n",
    "\n",
    "Proc_LogisticRegression     = config['MODELS']['LogisticRegression'    ]\n",
    "Proc_SVMGridSearch          = config['MODELS']['SVMGridSearch'         ]\n",
    "Proc_SVMRandomizedSearch    = config['MODELS']['SVMRandomizedSearch'   ]\n",
    "Proc_KNNearestNeighbors     = config['MODELS']['KNNearestNeighbors'    ]\n",
    "Proc_DecisionTreeClassifier = config['MODELS']['DecisionTreeClassifier']\n",
    "Proc_DeepNeuralNetworks     = config['MODELS']['DeepNeuralNetworks'    ]\n",
    "Proc_ConvolutionalNetworks  = config['MODELS']['ConvolutionalNetworks' ]\n",
    "Proc_LanguagePatternsAlg    = config['MODELS']['LanguagePatternsAlg'   ]\n",
    "Proc_BayesianOptimization   = config['MODELS']['BayesianOptimization'  ]\n",
    "\n",
    "print(f'LogisticRegression     : {Proc_LogisticRegression    }')\n",
    "print(f'SVMGridSearch          : {Proc_SVMGridSearch         }')\n",
    "print(f'SVMRandomizedSearch    : {Proc_SVMRandomizedSearch   }')\n",
    "print(f'KNNearestNeighbors     : {Proc_KNNearestNeighbors    }')\n",
    "print(f'DecisionTreeClassifier : {Proc_DecisionTreeClassifier}')\n",
    "print(f'DeepNeuralNetworks     : {Proc_DeepNeuralNetworks    }')\n",
    "print(f'ConvolutionNetworks    : {Proc_ConvolutionalNetworks }')\n",
    "print(f'LanguagePatternsAlg    : {Proc_LanguagePatternsAlg   }')\n",
    "print(f'BayesianOptimization   : {Proc_BayesianOptimization  }')\n",
    "\n",
    "model_outFile = config['DEFAULT']['model_outputFile'] + current_time + '.pkl'\n",
    "model_inFile  = config['DEFAULT']['model_inputFile' ] + current_time + '.pkl'\n",
    "model_prefix_    = config['DEFAULT']['model_prefix'    ]\n",
    "print(f'Model Output File: {model_outFile}')\n",
    "\n",
    "grid_search_verbose = int(config['PROCESS']['gridSearchVerbose'])\n",
    "print(f'grid_search_verbose: {grid_search_verbose}')\n",
    "\n",
    "dataset_split = float(config['DEFAULT']['train_test_split'])\n",
    "print(f'dataset_split: {dataset_split}')\n",
    "\n",
    "file_true = config['DATASET']['TrueFile']\n",
    "file_fake = config['DATASET']['FakeFile']\n",
    "\n",
    "nn_models_epochs          = 20\n",
    "nn_models_batchsize       = 128\n",
    "nn_models_validationSplit = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_fake input: (23481, 10)\n",
      "data_true input: (21417, 10)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Embedding, LSTM, GRU, Dense, Dropout, Bidirectional\n",
    "dataset_fraction = 1.0\n",
    "\n",
    "data_fake = pd.read_csv( file_fake ).sample(frac=dataset_fraction)\n",
    "data_true = pd.read_csv( file_true ).sample(frac=dataset_fraction)\n",
    "\n",
    "print(f'data_fake input: {data_fake.shape}')\n",
    "print(f'data_true input: {data_true.shape}')\n",
    "\n",
    "data_fake['class'] = 0\n",
    "data_true['class'] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom function to split text\n",
    "def split_text(text):\n",
    "    if '-' in text[:30]:\n",
    "        parts = text.split('-', 1)\n",
    "        return parts[0], parts[1]\n",
    "    else:\n",
    "        return 'None', text\n",
    "    \n",
    "data_fake[['publisher', 'text']] = data_fake['text'].apply(lambda x: pd.Series(split_text(x)))\n",
    "data_true[['publisher', 'text']] = data_true['text'].apply(lambda x: pd.Series(split_text(x)))\n",
    "\n",
    "num_fake = len(data_fake)\n",
    "num_true = len(data_true)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New DataFrame shape: (42834, 12)\n"
     ]
    }
   ],
   "source": [
    "# Find the minimum number of records\n",
    "min_records = min(num_fake, num_true)\n",
    "\n",
    "# Determine which DataFrame has more records\n",
    "if num_fake > num_true:\n",
    "    data_fake_sampled = data_fake.sample(n=min_records)\n",
    "    data = pd.concat([data_true, data_fake_sampled], ignore_index=True)\n",
    "    \n",
    "else:\n",
    "    data_true_sampled = data_true.sample(n=min_records)\n",
    "    data = pd.concat([data_fake, data_true_sampled], ignore_index=True)\n",
    "\n",
    "# Print the shape of the new DataFrame\n",
    "print(f'New DataFrame shape: {data.shape}')\n",
    "data = data.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "data = data.drop(columns=['title','subject','date','sentiment','emotion','objectivity','intent','assertions','publisher'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text filters for cleanup text passages\n",
    "def wordopt( text ):\n",
    "    text = text.lower()\n",
    "    text = re.sub('\\[.*?\\]','',text)\n",
    "    text = re.sub('\\\\W', ' ',  text)\n",
    "    text = re.sub('https?://\\S+|www\\.\\S+','',text)\n",
    "    text = re.sub('<.*?>+','',text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation),'',text)\n",
    "    text = re.sub('\\n','',text)\n",
    "    text = re.sub('\\w*\\d\\w*','',text)\n",
    "    return text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################################\n",
    "#  Data Preparation                                                      #\n",
    "#  Prepare the data frame in formats for each type of model used         #\n",
    "#  a) Split dataset into Train / Test                                    #\n",
    "#  b) TDIDF vectorization transformation of Training and Testing data    #\n",
    "#  c) Transform Training / Testing Data into Dense Arrays                #\n",
    "#  d) Reshape dense data arrays for input to RNN models                  #\n",
    "##########################################################################\n",
    "# Remove stop words, and escape characters\n",
    "data['text'] = data['text'].apply(wordopt)\n",
    "x = data[['text', 'summary']]\n",
    "y = data['class']\n",
    "\n",
    "Xv_0_train, Xv_0_test, y_train, y_test = train_test_split(x,y,test_size=dataset_split)\n",
    "Xv_train = Xv_0_train['summary']\n",
    "Xv_test  = Xv_0_test['text']\n",
    "\n",
    "\n",
    "max_features = 5000\n",
    "vectorization = TfidfVectorizer( max_features=max_features )\n",
    "X_train = vectorization.fit_transform( Xv_train )\n",
    "X_train_dense = X_train.toarray()\n",
    "\n",
    "X_test  = vectorization.transform( Xv_test )\n",
    "X_test_dense = X_test.toarray()\n",
    "\n",
    "y_train = np.array(y_train)\n",
    "y_test  = np.array(y_test )\n",
    "\n",
    "X_train_reshaped = X_train_dense.reshape((X_train_dense.shape[0], 1, X_train_dense.shape[1]))  # Shape: (num_samples, 1, num_features)\n",
    "X_test_reshaped  = X_test_dense.reshape((X_test_dense.shape[0],   1, X_test_dense.shape[1]))   # Shape: (num_samples, 1, num_features)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Dropout, Bidirectional, LSTM, GRU, Dense\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "max_words = 30000           # Maximum number of words to consider in vocabulary\n",
    "max_len   = 2000            # Maximum length of sequences (padding/truncating)\n",
    "\n",
    "embedding_dim = 128         # Dimension of word embedding\n",
    "\n",
    "model_results = []\n",
    "model_results_headers = ['Model','Training Time','Testing Time','Accuracy','Precision(weighted)','Recall(weighted)','f1-score(weighted)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN Models\n",
    "#   a) BiLSTM, BiGRU - BiDirectional  64-LSTM, 32-GRU hidden layers\n",
    "#   b) LSTM, GRU     - UniDirectional 64-LSTM, 32-GRU hidden layers\n",
    "\n",
    "from keras.models import load_model, Sequential, Model\n",
    "from keras.layers import Dense, Conv1D, MaxPooling1D, LSTM, Dropout, Flatten, Input, Embedding\n",
    "from keras.layers import Bidirectional\n",
    "\n",
    "def create_model_Rnn_BiLSTM_BiGRU( modelType, max_words=max_words, embedding_dim=embedding_dim ):\n",
    "    print(f'Creating model type: {modelType}')\n",
    "    model = Sequential([\n",
    "        Dropout(0.2),\n",
    "        Bidirectional(LSTM(64, return_sequences=True)),\n",
    "        Dropout(0.2),\n",
    "        Bidirectional(GRU(32)),\n",
    "        Dropout(0.2),\n",
    "        Dense(4, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(1, activation='sigmoid')  # Sigmoid for binary classification\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def create_model_Rnn_LSTM_GRU( modelType, max_words=max_words, embedding_dim=embedding_dim ):\n",
    "    print(f'Creating model type: {modelType}')\n",
    "    model = Sequential([\n",
    "        Dropout(0.2),\n",
    "        LSTM(64, return_sequences=True),\n",
    "        Dropout(0.2),\n",
    "        GRU(32),\n",
    "        Dropout(0.2),\n",
    "        Dense(4, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# CNN Models\n",
    "#   a) CNN + RNN + Transformer\n",
    "\n",
    "def create_model_CnnRnnTransformer(input_shape):\n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    # CNN part\n",
    "    cnn_layer = Conv1D(filters=128, kernel_size=5, activation='relu')(inputs)\n",
    "    cnn_layer = MaxPooling1D(pool_size=2)(cnn_layer)\n",
    "    cnn_layer = Conv1D(filters=64, kernel_size=5, activation='relu')(cnn_layer)\n",
    "    cnn_layer = MaxPooling1D(pool_size=2)(cnn_layer)\n",
    "\n",
    "    # RNN (LSTM or GRU)\n",
    "    rnn_layer = LSTM(128, return_sequences=True)(cnn_layer)\n",
    "    rnn_layer = Dropout(0.2)(rnn_layer)\n",
    "\n",
    "    # Optionally, use Bidirectional LSTM/GRU for better performance\n",
    "    rnn_layer = Bidirectional(LSTM(128,return_sequences=True))(rnn_layer)\n",
    "\n",
    "    # Transformer Layer using MultiHeadAttention\n",
    "    transformer_layer = tf.keras.layers.MultiHeadAttention(num_heads=4, key_dim=128)(rnn_layer, rnn_layer)\n",
    "    \n",
    "    # Pooling to reduce the sequence to a fixed size (optional global average pooling)\n",
    "    pooled_output = tf.keras.layers.GlobalAveragePooling1D()(transformer_layer)\n",
    "\n",
    "    # Fully Connected Layer\n",
    "    dense_layer = Dense(64, activation='relu')(pooled_output)\n",
    "    dense_layer = Dropout(0.5)(dense_layer)\n",
    "\n",
    "    # Output Layer (assuming binary classification)\n",
    "    outputs = Dense(1, activation='sigmoid')(dense_layer)\n",
    "\n",
    "    # Define the model\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating model type: RNN_BiLSTM_BiGRU\n",
      "Epoch 1/20\n",
      "175/175 [==============================] - 9s 16ms/step - loss: 0.4785 - accuracy: 0.8336 - val_loss: 0.3853 - val_accuracy: 0.9384\n",
      "Epoch 2/20\n",
      "175/175 [==============================] - 2s 10ms/step - loss: 0.3611 - accuracy: 0.9384 - val_loss: 0.3536 - val_accuracy: 0.9382\n",
      "Epoch 3/20\n",
      "175/175 [==============================] - 2s 11ms/step - loss: 0.3205 - accuracy: 0.9431 - val_loss: 0.3337 - val_accuracy: 0.9343\n",
      "Epoch 4/20\n",
      "175/175 [==============================] - 2s 10ms/step - loss: 0.2885 - accuracy: 0.9479 - val_loss: 0.3219 - val_accuracy: 0.9348\n",
      "Epoch 5/20\n",
      "175/175 [==============================] - 2s 10ms/step - loss: 0.2652 - accuracy: 0.9503 - val_loss: 0.3194 - val_accuracy: 0.9343\n",
      "Epoch 6/20\n",
      "175/175 [==============================] - 2s 11ms/step - loss: 0.2455 - accuracy: 0.9518 - val_loss: 0.2962 - val_accuracy: 0.9393\n",
      "Epoch 7/20\n",
      "175/175 [==============================] - 2s 10ms/step - loss: 0.2277 - accuracy: 0.9532 - val_loss: 0.2802 - val_accuracy: 0.9366\n",
      "Epoch 8/20\n",
      "175/175 [==============================] - 2s 11ms/step - loss: 0.2135 - accuracy: 0.9541 - val_loss: 0.2717 - val_accuracy: 0.9389\n",
      "Epoch 9/20\n",
      "175/175 [==============================] - 2s 11ms/step - loss: 0.2005 - accuracy: 0.9558 - val_loss: 0.2852 - val_accuracy: 0.9368\n",
      "Epoch 10/20\n",
      "175/175 [==============================] - 2s 11ms/step - loss: 0.1895 - accuracy: 0.9574 - val_loss: 0.2698 - val_accuracy: 0.9380\n",
      "Epoch 11/20\n",
      "175/175 [==============================] - 2s 10ms/step - loss: 0.1832 - accuracy: 0.9578 - val_loss: 0.2480 - val_accuracy: 0.9375\n",
      "Epoch 12/20\n",
      "175/175 [==============================] - 2s 11ms/step - loss: 0.1765 - accuracy: 0.9572 - val_loss: 0.2380 - val_accuracy: 0.9384\n",
      "Epoch 13/20\n",
      "175/175 [==============================] - 2s 11ms/step - loss: 0.1682 - accuracy: 0.9596 - val_loss: 0.2465 - val_accuracy: 0.9380\n",
      "Epoch 14/20\n",
      "175/175 [==============================] - 2s 11ms/step - loss: 0.1619 - accuracy: 0.9602 - val_loss: 0.2554 - val_accuracy: 0.9361\n",
      "Epoch 15/20\n",
      "175/175 [==============================] - 2s 10ms/step - loss: 0.1603 - accuracy: 0.9580 - val_loss: 0.2445 - val_accuracy: 0.9386\n",
      "Epoch 16/20\n",
      "175/175 [==============================] - 2s 11ms/step - loss: 0.1518 - accuracy: 0.9608 - val_loss: 0.2451 - val_accuracy: 0.9384\n",
      "Epoch 17/20\n",
      "175/175 [==============================] - 2s 11ms/step - loss: 0.1478 - accuracy: 0.9609 - val_loss: 0.2516 - val_accuracy: 0.9368\n",
      "Epoch 18/20\n",
      "175/175 [==============================] - 2s 10ms/step - loss: 0.1455 - accuracy: 0.9605 - val_loss: 0.2592 - val_accuracy: 0.9363\n",
      "Epoch 19/20\n",
      "175/175 [==============================] - 2s 12ms/step - loss: 0.1409 - accuracy: 0.9616 - val_loss: 0.2395 - val_accuracy: 0.9380\n",
      "Epoch 20/20\n",
      "175/175 [==============================] - 2s 10ms/step - loss: 0.1379 - accuracy: 0.9629 - val_loss: 0.2407 - val_accuracy: 0.9379\n",
      "\n",
      "Training Time: 45.44 seconds\n",
      "\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.6689 - accuracy: 0.8985\n",
      "Test Accuracy: 0.8985\n",
      "469/469 [==============================] - 2s 4ms/step\n",
      "rnn_bidirectional_sum_txt.h5: Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.96      0.91      7585\n",
      "           1       0.96      0.83      0.89      7407\n",
      "\n",
      "    accuracy                           0.90     14992\n",
      "   macro avg       0.91      0.90      0.90     14992\n",
      "weighted avg       0.91      0.90      0.90     14992\n",
      "\n",
      "rnn_bidirectional_sum_txt.h5: Accuracy Score: 0.8985\n",
      "Creating model type: RNN_BiLSTM_BiGRU\n",
      "Epoch 1/20\n",
      "175/175 [==============================] - 5s 12ms/step - loss: 0.4824 - accuracy: 0.7740 - val_loss: 0.1890 - val_accuracy: 0.9341\n",
      "Epoch 2/20\n",
      "175/175 [==============================] - 1s 8ms/step - loss: 0.2189 - accuracy: 0.9336 - val_loss: 0.1540 - val_accuracy: 0.9416\n",
      "Epoch 3/20\n",
      "175/175 [==============================] - 1s 8ms/step - loss: 0.1833 - accuracy: 0.9433 - val_loss: 0.1517 - val_accuracy: 0.9418\n",
      "Epoch 4/20\n",
      "175/175 [==============================] - 1s 7ms/step - loss: 0.1648 - accuracy: 0.9504 - val_loss: 0.1535 - val_accuracy: 0.9400\n",
      "Epoch 5/20\n",
      "175/175 [==============================] - 1s 7ms/step - loss: 0.1536 - accuracy: 0.9512 - val_loss: 0.1486 - val_accuracy: 0.9406\n",
      "Epoch 6/20\n",
      "175/175 [==============================] - 1s 7ms/step - loss: 0.1432 - accuracy: 0.9551 - val_loss: 0.1471 - val_accuracy: 0.9404\n",
      "Epoch 7/20\n",
      "175/175 [==============================] - 1s 8ms/step - loss: 0.1323 - accuracy: 0.9552 - val_loss: 0.1529 - val_accuracy: 0.9400\n",
      "Epoch 8/20\n",
      "175/175 [==============================] - 1s 7ms/step - loss: 0.1283 - accuracy: 0.9574 - val_loss: 0.1546 - val_accuracy: 0.9384\n",
      "Epoch 9/20\n",
      "175/175 [==============================] - 1s 8ms/step - loss: 0.1214 - accuracy: 0.9593 - val_loss: 0.1600 - val_accuracy: 0.9379\n",
      "Epoch 10/20\n",
      "175/175 [==============================] - 1s 8ms/step - loss: 0.1232 - accuracy: 0.9572 - val_loss: 0.1598 - val_accuracy: 0.9391\n",
      "Epoch 11/20\n",
      "175/175 [==============================] - 2s 9ms/step - loss: 0.1161 - accuracy: 0.9586 - val_loss: 0.1685 - val_accuracy: 0.9393\n",
      "Epoch 12/20\n",
      "175/175 [==============================] - 1s 8ms/step - loss: 0.1194 - accuracy: 0.9572 - val_loss: 0.1687 - val_accuracy: 0.9389\n",
      "Epoch 13/20\n",
      "175/175 [==============================] - 1s 8ms/step - loss: 0.1146 - accuracy: 0.9593 - val_loss: 0.1749 - val_accuracy: 0.9372\n",
      "Epoch 14/20\n",
      "175/175 [==============================] - 1s 8ms/step - loss: 0.1108 - accuracy: 0.9595 - val_loss: 0.1880 - val_accuracy: 0.9366\n",
      "Epoch 15/20\n",
      "175/175 [==============================] - 1s 8ms/step - loss: 0.1114 - accuracy: 0.9587 - val_loss: 0.1909 - val_accuracy: 0.9368\n",
      "Epoch 16/20\n",
      "175/175 [==============================] - 1s 7ms/step - loss: 0.1123 - accuracy: 0.9583 - val_loss: 0.1866 - val_accuracy: 0.9375\n",
      "Epoch 17/20\n",
      "175/175 [==============================] - 1s 8ms/step - loss: 0.1098 - accuracy: 0.9591 - val_loss: 0.1946 - val_accuracy: 0.9382\n",
      "Epoch 18/20\n",
      "175/175 [==============================] - 1s 8ms/step - loss: 0.1070 - accuracy: 0.9615 - val_loss: 0.1851 - val_accuracy: 0.9373\n",
      "Epoch 19/20\n",
      "175/175 [==============================] - 2s 9ms/step - loss: 0.1070 - accuracy: 0.9603 - val_loss: 0.1927 - val_accuracy: 0.9388\n",
      "Epoch 20/20\n",
      "175/175 [==============================] - 1s 8ms/step - loss: 0.1048 - accuracy: 0.9610 - val_loss: 0.1924 - val_accuracy: 0.9373\n",
      "\n",
      "Training Time: 31.43 seconds\n",
      "\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.5272 - accuracy: 0.9037\n",
      "Test Accuracy: 0.9037\n",
      "469/469 [==============================] - 1s 2ms/step\n",
      "rnn_unidirectional_sum_txt.h5: Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.97      0.91      7585\n",
      "           1       0.96      0.84      0.90      7407\n",
      "\n",
      "    accuracy                           0.90     14992\n",
      "   macro avg       0.91      0.90      0.90     14992\n",
      "weighted avg       0.91      0.90      0.90     14992\n",
      "\n",
      "rnn_unidirectional_sum_txt.h5: Accuracy Score: 0.9037\n"
     ]
    }
   ],
   "source": [
    "############################################################################################\n",
    "#  Executing Neural Network Models - a) RNN + BiLSTM  + BiGRU                              #\n",
    "#                                    b) RNN + UniLSTM + UniGRU                             #\n",
    "#  epcochs   : 20                                                                          #\n",
    "#  batch_size: 128                                                                         #\n",
    "############################################################################################\n",
    "\n",
    "import os\n",
    "import joblib\n",
    "from keras.models import load_model\n",
    "\n",
    "models = { model_rnn_bidirectional_fname: create_model_Rnn_BiLSTM_BiGRU, model_rnn_unidirectional_fname: create_model_Rnn_LSTM_GRU }\n",
    "\n",
    "for model_name, model_function in models.items():\n",
    "    if( os.path.exists( model_name ) ):\n",
    "        model = load_model( model_name )\n",
    "        print(f'model: {model_name} loaded ...')\n",
    "    else:\n",
    "        model = model_function(\"RNN_BiLSTM_BiGRU\", max_words=max_words, embedding_dim=embedding_dim)\n",
    "\n",
    "    start_time = time.time()\n",
    "    history = model.fit(X_train_reshaped, y_train, \n",
    "                        epochs=nn_models_epochs, \n",
    "                        batch_size=nn_models_batchsize, \n",
    "                        validation_split=nn_models_validationSplit, \n",
    "                        verbose=1)\n",
    "    train_time = time.time() - start_time\n",
    "    print(f'\\nTraining Time: {train_time:.2f} seconds\\n')\n",
    "    start_time = time.time()\n",
    "    test_loss, test_accuracy = model.evaluate(X_test_reshaped, y_test)\n",
    "    \n",
    "    test_time = time.time() - start_time\n",
    "    print(f'Test Accuracy: {test_accuracy:.4f}')\n",
    "\n",
    "    y_pred = (model.predict(X_test_reshaped) > 0.50).astype(\"int32\")\n",
    "    print(f\"{model_name}: Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "    print(f\"{model_name}: Accuracy Score: {accuracy_score(y_test, y_pred):.4f}\")    \n",
    "    \n",
    "    report = classification_report(y_test,y_pred,output_dict=True)\n",
    "    precision = report['weighted avg']['precision']\n",
    "    recall    = report['weighted avg']['recall']\n",
    "    f1_score  = report['weighted avg']['f1-score']\n",
    "\n",
    "    model_results.append([model_name, f'{train_time:.2f} sec', f'{test_time:.2f} sec', f'{test_accuracy:.4f}',f'{precision:.4f}',f'{recall:.4f}',f'{f1_score:.4f}'])\n",
    "    model.save(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_name: randforest_sum_txt.h5\n",
      "model_name: adaboost_sum_txt.h5\n",
      "model_name: gradboost_sum_txt.h5\n",
      "#####  randforest_sum_txt.h5  #####\n",
      "regular training fit\n",
      "Training Time: 5.13 seconds\n",
      "\n",
      "[[7559   26]\n",
      " [5708 1699]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      1.00      0.73      7585\n",
      "           1       0.98      0.23      0.37      7407\n",
      "\n",
      "    accuracy                           0.62     14992\n",
      "   macro avg       0.78      0.61      0.55     14992\n",
      "weighted avg       0.77      0.62      0.55     14992\n",
      "\n",
      "#####  adaboost_sum_txt.h5  #####\n",
      "regular training fit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fkao9\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Time: 25.39 seconds\n",
      "\n",
      "[[7011  574]\n",
      " [4259 3148]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.92      0.74      7585\n",
      "           1       0.85      0.43      0.57      7407\n",
      "\n",
      "    accuracy                           0.68     14992\n",
      "   macro avg       0.73      0.67      0.65     14992\n",
      "weighted avg       0.73      0.68      0.66     14992\n",
      "\n",
      "#####  gradboost_sum_txt.h5  #####\n",
      "regular training fit\n",
      "Training Time: 59.41 seconds\n",
      "\n",
      "[[7436  149]\n",
      " [6344 1063]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.98      0.70      7585\n",
      "           1       0.88      0.14      0.25      7407\n",
      "\n",
      "    accuracy                           0.57     14992\n",
      "   macro avg       0.71      0.56      0.47     14992\n",
      "weighted avg       0.71      0.57      0.47     14992\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "models_cpu = {\n",
    "    model_randforest_fname : RandomForestClassifier,\n",
    "    model_adaboost_fname   : AdaBoostClassifier,\n",
    "#    model_bagging_fname    : BaggingClassifier,\n",
    "    model_gradboost_fname  : GradientBoostingClassifier\n",
    "}\n",
    "for model_name, model_ in models_cpu.items():\n",
    "    print(f'model_name: {model_name}')\n",
    "\n",
    "model_n_estimators=10\n",
    "for model_name, model_function in models_cpu.items():\n",
    "    print(f'#####  {model_name}  #####')\n",
    "\n",
    "    if( os.path.exists( model_name ) ):\n",
    "        model = joblib.load( model_name )\n",
    "        print(f'model: {model_name} loaded ...')\n",
    "    else:\n",
    "        model = model_function(n_estimators=model_n_estimators)\n",
    "    \n",
    "    # Check if the model supports partial fitting\n",
    "    start_time = time.time()\n",
    "    if hasattr(model, 'partial_fit'):\n",
    "        # For partial fit, we need to specify classes for classification models\n",
    "        print(f'partial training fit')\n",
    "        classes = np.unique(y_train)\n",
    "        model.partial_fit(X_train_dense, y_train, classes=classes)\n",
    "    else:\n",
    "        print(f'regular training fit')\n",
    "        model.fit(X_train_dense, y_train)      \n",
    "    \n",
    "    train_time = time.time() - start_time\n",
    "    print(f'Training Time: {train_time:.2f} seconds\\n')\n",
    "    start_time = time.time()\n",
    "    y_pred = model.predict(X_test_dense)\n",
    "    test_time = time.time() - start_time\n",
    "    \n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    print(f'{conf_matrix}')\n",
    "    print(f'{classification_report(y_test, y_pred)}')\n",
    "\n",
    "    test_accuracy = accuracy_score(y_test,y_pred)\n",
    "    \n",
    "    report = classification_report(y_test,y_pred,output_dict=True)\n",
    "    precision = report['weighted avg']['precision']\n",
    "    recall    = report['weighted avg']['recall']\n",
    "    f1_score  = report['weighted avg']['f1-score']\n",
    "\n",
    "    model_results.append([model_name, f'{train_time:.2f} sec', f'{test_time:.2f} sec', f'{test_accuracy:.4f}',f'{precision:.4f}',f'{recall:.4f}',f'{f1_score:.4f}'])\n",
    "   \n",
    "    joblib.dump( model, model_name )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fkao9\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [20:12:32] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"predictor\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Time: 2.17 seconds\n",
      "\n",
      "Confusion Matrix for XGBoost (GPU):\n",
      "[[7546   39]\n",
      " [5300 2107]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.99      0.74      7585\n",
      "           1       0.98      0.28      0.44      7407\n",
      "\n",
      "    accuracy                           0.64     14992\n",
      "   macro avg       0.78      0.64      0.59     14992\n",
      "weighted avg       0.78      0.64      0.59     14992\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['xgboost_sum_txt.h5']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##### XgBoost - CUDA #####\n",
    "import xgboost as xgb\n",
    "import os\n",
    "\n",
    "model_name = model_xgboost_fname\n",
    "\n",
    "dtrain = xgb.DMatrix(X_train_dense, label=y_train)\n",
    "dtest  = xgb.DMatrix(X_test_dense,  label=y_test )\n",
    "\n",
    "start_time = time.time()\n",
    "params = {\n",
    "    'objective': 'binary:logistic',  # Assuming binary classification\n",
    "    'tree_method': 'hist',           # Use GPU for training\n",
    "    'device'     : 'cuda',           # Use CUDA for GPU\n",
    "    'max_depth'  : 4,\n",
    "    'predictor'  : 'cpu_predictor'   # GPU for prediction as well\n",
    "}\n",
    "\n",
    "model = xgb.train(params, dtrain, num_boost_round=50)\n",
    "train_time = time.time() - start_time\n",
    "print(f'Training Time: {train_time:.2f} seconds\\n')\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "y_pred = model.predict(dtest)\n",
    "y_pred = [1 if pred > 0.5 else 0 for pred in y_pred]  # Convert probabilities to class labels\n",
    "\n",
    "test_time = time.time() - start_time\n",
    "\n",
    "# Confusion matrix and classification report\n",
    "print(\"Confusion Matrix for XGBoost (GPU):\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "test_accuracy = accuracy_score(y_test,y_pred)\n",
    "\n",
    "report = classification_report(y_test,y_pred,output_dict=True)\n",
    "precision = report['weighted avg']['precision']\n",
    "recall    = report['weighted avg']['recall']\n",
    "f1_score  = report['weighted avg']['f1-score']\n",
    "\n",
    "model_results.append([model_name, f'{train_time:.2f} sec', f'{test_time:.2f} sec', f'{test_accuracy:.4f}',f'{precision:.4f}',f'{recall:.4f}',f'{f1_score:.4f}'])\n",
    "\n",
    "joblib.dump( model,model_name )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ModelClassification_GridSearch(classifier, classifier_text, X_train,y_train,X_test,y_test,param_grid):\n",
    "    start_time = time.time()\n",
    "\n",
    "    grid_search = GridSearchCV(classifier, param_grid, cv=10, n_jobs=-1, scoring='accuracy',verbose=grid_search_verbose)\n",
    "    model_fit_param = grid_search.fit(X_train, y_train)\n",
    "    best_regressor  = grid_search.best_estimator_\n",
    "\n",
    "    train_time = time.time() - start_time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    y_pred                 = best_regressor.predict(X_test)\n",
    "    \n",
    "    test_time = time.time() - start_time\n",
    "    \n",
    "    model_accuracy         = accuracy_score(y_test,y_pred)\n",
    "    model_confusion_matrix = confusion_matrix(y_test,y_pred)\n",
    "    \n",
    "    report_classification = classification_report(y_test,y_pred, output_dict=True)\n",
    "\n",
    "    grid_training_score = best_regressor.score(X_train,y_train)\n",
    "    grid_testing_score  = best_regressor.score(X_test, y_test )\n",
    "\n",
    "    precision = report_classification['weighted avg']['precision']\n",
    "    recall    = report_classification['weighted avg']['recall']\n",
    "    f1_score  = report_classification['weighted avg']['f1-score']\n",
    "\n",
    "    model_results.append([classifier_text, f'{train_time:.2f} sec', f'{test_time:.2f} sec', f'{test_accuracy:.4f}',f'{precision:.4f}',f'{recall:.4f}',f'{f1_score:.4f}'])\n",
    "    \n",
    "    joblib.dump( best_regressor, classifier_text )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 9 candidates, totalling 90 fits\n",
      "Fitting 10 folds for each of 6 candidates, totalling 60 fits\n"
     ]
    }
   ],
   "source": [
    "if Proc_SVMGridSearch == 'TRUE':\n",
    "    param_grid_svc = {\n",
    "        'C': [0.1,1, 10], \n",
    "        'gamma': [1,0.1,0.01]\n",
    "    } \n",
    "    classifier_gridsearch_report = ModelClassification_GridSearch(SVC(), 'SVMGridSearch', X_train, y_train, X_test, y_test, param_grid_svc) \n",
    "\n",
    "if Proc_LogisticRegression == 'TRUE':\n",
    "    param_grid_lr = {\n",
    "        'C': [0.1, 1, 10],\n",
    "        'solver': ['liblinear', 'saga']\n",
    "    }\n",
    "    classifier_gridsearch_report = ModelClassification_GridSearch(LogisticRegression(), 'Logistic Regression GridSearch', X_train, y_train, X_test, y_test, param_grid_lr)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+---------------+--------------+----------+---------------------+------------------+--------------------+\n",
      "|             Model              | Training Time | Testing Time | Accuracy | Precision(weighted) | Recall(weighted) | f1-score(weighted) |\n",
      "+--------------------------------+---------------+--------------+----------+---------------------+------------------+--------------------+\n",
      "|  rnn_bidirectional_sum_txt.h5  |   45.44 sec   |   2.34 sec   |  0.8985  |       0.9051        |      0.8985      |       0.8980       |\n",
      "| rnn_unidirectional_sum_txt.h5  |   31.43 sec   |   1.68 sec   |  0.9037  |       0.9099        |      0.9037      |       0.9032       |\n",
      "|     randforest_sum_txt.h5      |   5.13 sec    |   0.22 sec   |  0.6175  |       0.7749        |      0.6175      |       0.5507       |\n",
      "|      adaboost_sum_txt.h5       |   25.39 sec   |   1.39 sec   |  0.6776  |       0.7326        |      0.6776      |       0.6558       |\n",
      "|      gradboost_sum_txt.h5      |   59.41 sec   |   0.18 sec   |  0.5669  |       0.7063        |      0.5669      |       0.4740       |\n",
      "|       xgboost_sum_txt.h5       |   2.17 sec    |   0.10 sec   |  0.6439  |       0.7823        |      0.6439      |       0.5917       |\n",
      "|         SVMGridSearch          |  2925.44 sec  |  75.27 sec   |  0.6439  |       0.9021        |      0.8821      |       0.8804       |\n",
      "| Logistic Regression GridSearch |   11.56 sec   |   0.01 sec   |  0.6439  |       0.9588        |      0.9567      |       0.9566       |\n",
      "+--------------------------------+---------------+--------------+----------+---------------------+------------------+--------------------+\n"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "print( tabulate(model_results, model_results_headers, tablefmt='pretty') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
