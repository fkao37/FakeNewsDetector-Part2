{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU is available\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration Sections: ['DATASET', 'FEATURE_PROCESSING', 'EDA', 'PROCESS', 'MODELS']\n",
      "DropFeatures: 'title','subject','date', type: <class 'str'>\n",
      "LogisticRegression     : TRUE\n",
      "SVMGridSearch          : TRUE\n",
      "SVMRandomizedSearch    : TRUE\n",
      "KNNearestNeighbors     : FALSE\n",
      "DecisionTreeClassifier : TRUE\n",
      "DeepNeuralNetworks     : FALSE\n",
      "ConvolutionNetworks    : FALSE\n",
      "LanguagePatternsAlg    : FALSE\n",
      "BayesianOptimization   : FALSE\n",
      "Model Output File: FakeNewsModel_1728521642.8647237.pkl\n",
      "grid_search_verbose: 3\n",
      "dataset_split: 0.35\n"
     ]
    }
   ],
   "source": [
    "import configparser\n",
    "import time\n",
    "\n",
    "current_time = str(time.time())\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read('FakeNewsModel_txt_sum.ini')\n",
    "print(f'Configuration Sections: {config.sections()}')\n",
    "\n",
    "model_rnn_bidirectional_fname  = config['DEFAULT']['model_rnn_bidirectional_fname']\n",
    "model_rnn_unidirectional_fname = config['DEFAULT']['model_rnn_unidirectional_fname']\n",
    "\n",
    "model_cnn_fname  = config['DEFAULT']['model_cnn_fname']\n",
    "model_randforest_fname = config['DEFAULT']['model_randforest_fname']\n",
    "model_adaboost_fname   = config['DEFAULT']['model_adaboost_fname']\n",
    "model_xgboost_fname    = config['DEFAULT']['model_xgboost_fname']\n",
    "model_bagging_fname    = config['DEFAULT']['model_bagging_fname']\n",
    "model_gradboost_fname  = config['DEFAULT']['model_gradboost_fname']\n",
    "model_svm_fname        = config['DEFAULT']['model_svm_fname']\n",
    "model_lgr_fname        = config['DEFAULT']['model_lgr_fname']\n",
    "\n",
    "\n",
    "DropFeatures = config['FEATURE_PROCESSING']['DropFeatures']\n",
    "print(f'DropFeatures: {DropFeatures}, type: {type(DropFeatures)}')\n",
    "\n",
    "Proc_LogisticRegression     = config['MODELS']['LogisticRegression'    ]\n",
    "Proc_SVMGridSearch          = config['MODELS']['SVMGridSearch'         ]\n",
    "Proc_SVMRandomizedSearch    = config['MODELS']['SVMRandomizedSearch'   ]\n",
    "Proc_KNNearestNeighbors     = config['MODELS']['KNNearestNeighbors'    ]\n",
    "Proc_DecisionTreeClassifier = config['MODELS']['DecisionTreeClassifier']\n",
    "Proc_DeepNeuralNetworks     = config['MODELS']['DeepNeuralNetworks'    ]\n",
    "Proc_ConvolutionalNetworks  = config['MODELS']['ConvolutionalNetworks' ]\n",
    "Proc_LanguagePatternsAlg    = config['MODELS']['LanguagePatternsAlg'   ]\n",
    "Proc_BayesianOptimization   = config['MODELS']['BayesianOptimization'  ]\n",
    "\n",
    "print(f'LogisticRegression     : {Proc_LogisticRegression    }')\n",
    "print(f'SVMGridSearch          : {Proc_SVMGridSearch         }')\n",
    "print(f'SVMRandomizedSearch    : {Proc_SVMRandomizedSearch   }')\n",
    "print(f'KNNearestNeighbors     : {Proc_KNNearestNeighbors    }')\n",
    "print(f'DecisionTreeClassifier : {Proc_DecisionTreeClassifier}')\n",
    "print(f'DeepNeuralNetworks     : {Proc_DeepNeuralNetworks    }')\n",
    "print(f'ConvolutionNetworks    : {Proc_ConvolutionalNetworks }')\n",
    "print(f'LanguagePatternsAlg    : {Proc_LanguagePatternsAlg   }')\n",
    "print(f'BayesianOptimization   : {Proc_BayesianOptimization  }')\n",
    "\n",
    "model_outFile = config['DEFAULT']['model_outputFile'] + current_time + '.pkl'\n",
    "model_inFile  = config['DEFAULT']['model_inputFile' ] + current_time + '.pkl'\n",
    "model_prefix_    = config['DEFAULT']['model_prefix'    ]\n",
    "print(f'Model Output File: {model_outFile}')\n",
    "\n",
    "grid_search_verbose = int(config['PROCESS']['gridSearchVerbose'])\n",
    "print(f'grid_search_verbose: {grid_search_verbose}')\n",
    "\n",
    "dataset_split = float(config['DEFAULT']['train_test_split'])\n",
    "print(f'dataset_split: {dataset_split}')\n",
    "\n",
    "file_true = config['DATASET']['TrueFile']\n",
    "file_fake = config['DATASET']['FakeFile']\n",
    "\n",
    "nn_models_epochs          = 20\n",
    "nn_models_batchsize       = 128\n",
    "nn_models_validationSplit = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_fake input: (23481, 10)\n",
      "data_true input: (21417, 10)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Embedding, LSTM, GRU, Dense, Dropout, Bidirectional\n",
    "dataset_fraction = 1.0\n",
    "\n",
    "data_fake = pd.read_csv( file_fake ).sample(frac=dataset_fraction)\n",
    "data_true = pd.read_csv( file_true ).sample(frac=dataset_fraction)\n",
    "\n",
    "print(f'data_fake input: {data_fake.shape}')\n",
    "print(f'data_true input: {data_true.shape}')\n",
    "\n",
    "data_fake['class'] = 0\n",
    "data_true['class'] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom function to split text\n",
    "def split_text(text):\n",
    "    if '-' in text[:30]:\n",
    "        parts = text.split('-', 1)\n",
    "        return parts[0], parts[1]\n",
    "    else:\n",
    "        return 'None', text\n",
    "    \n",
    "data_fake[['publisher', 'text']] = data_fake['text'].apply(lambda x: pd.Series(split_text(x)))\n",
    "data_true[['publisher', 'text']] = data_true['text'].apply(lambda x: pd.Series(split_text(x)))\n",
    "\n",
    "num_fake = len(data_fake)\n",
    "num_true = len(data_true)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New DataFrame shape: (42834, 12)\n"
     ]
    }
   ],
   "source": [
    "# Find the minimum number of records\n",
    "min_records = min(num_fake, num_true)\n",
    "\n",
    "# Determine which DataFrame has more records\n",
    "if num_fake > num_true:\n",
    "    data_fake_sampled = data_fake.sample(n=min_records)\n",
    "    data = pd.concat([data_true, data_fake_sampled], ignore_index=True)\n",
    "    \n",
    "else:\n",
    "    data_true_sampled = data_true.sample(n=min_records)\n",
    "    data = pd.concat([data_fake, data_true_sampled], ignore_index=True)\n",
    "\n",
    "# Print the shape of the new DataFrame\n",
    "print(f'New DataFrame shape: {data.shape}')\n",
    "data = data.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "data = data.drop(columns=['title','subject','date','sentiment','emotion','objectivity','intent','assertions','publisher'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text filters for cleanup text passages\n",
    "def wordopt( text ):\n",
    "    text = text.lower()\n",
    "    text = re.sub('\\[.*?\\]','',text)\n",
    "    text = re.sub('\\\\W', ' ',  text)\n",
    "    text = re.sub('https?://\\S+|www\\.\\S+','',text)\n",
    "    text = re.sub('<.*?>+','',text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation),'',text)\n",
    "    text = re.sub('\\n','',text)\n",
    "    text = re.sub('\\w*\\d\\w*','',text)\n",
    "    return text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################################\n",
    "#  Data Preparation                                                      #\n",
    "#  Prepare the data frame in formats for each type of model used         #\n",
    "#  a) Split dataset into Train / Test                                    #\n",
    "#  b) TDIDF vectorization transformation of Training and Testing data    #\n",
    "#  c) Transform Training / Testing Data into Dense Arrays                #\n",
    "#  d) Reshape dense data arrays for input to RNN models                  #\n",
    "##########################################################################\n",
    "# Remove stop words, and escape characters\n",
    "data['text'] = data['text'].apply(wordopt)\n",
    "x = data[['text', 'summary']]\n",
    "y = data['class']\n",
    "\n",
    "Xv_0_train, Xv_0_test, y_train, y_test = train_test_split(x,y,test_size=dataset_split)\n",
    "Xv_train = Xv_0_train['text']\n",
    "Xv_test  = Xv_0_test['summary']\n",
    "\n",
    "\n",
    "max_features = 5000\n",
    "vectorization = TfidfVectorizer( max_features=max_features )\n",
    "X_train = vectorization.fit_transform( Xv_train )\n",
    "X_train_dense = X_train.toarray()\n",
    "\n",
    "X_test  = vectorization.transform( Xv_test )\n",
    "X_test_dense = X_test.toarray()\n",
    "\n",
    "y_train = np.array(y_train)\n",
    "y_test  = np.array(y_test )\n",
    "\n",
    "X_train_reshaped = X_train_dense.reshape((X_train_dense.shape[0], 1, X_train_dense.shape[1]))  # Shape: (num_samples, 1, num_features)\n",
    "X_test_reshaped  = X_test_dense.reshape((X_test_dense.shape[0],   1, X_test_dense.shape[1]))   # Shape: (num_samples, 1, num_features)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Dropout, Bidirectional, LSTM, GRU, Dense\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "max_words = 30000           # Maximum number of words to consider in vocabulary\n",
    "max_len   = 2000            # Maximum length of sequences (padding/truncating)\n",
    "\n",
    "embedding_dim = 128         # Dimension of word embedding\n",
    "\n",
    "model_results = []\n",
    "model_results_headers = ['Model','Training Time','Testing Time','Accuracy','Precision(weighted)','Recall(weighted)','f1-score(weighted)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN Models\n",
    "#   a) BiLSTM, BiGRU - BiDirectional  64-LSTM, 32-GRU hidden layers\n",
    "#   b) LSTM, GRU     - UniDirectional 64-LSTM, 32-GRU hidden layers\n",
    "\n",
    "from keras.models import load_model, Sequential, Model\n",
    "from keras.layers import Dense, Conv1D, MaxPooling1D, LSTM, Dropout, Flatten, Input, Embedding\n",
    "from keras.layers import Bidirectional\n",
    "\n",
    "def create_model_Rnn_BiLSTM_BiGRU( modelType, max_words=max_words, embedding_dim=embedding_dim ):\n",
    "    print(f'Creating model type: {modelType}')\n",
    "    model = Sequential([\n",
    "        Dropout(0.2),\n",
    "        Bidirectional(LSTM(64, return_sequences=True)),\n",
    "        Dropout(0.2),\n",
    "        Bidirectional(GRU(32)),\n",
    "        Dropout(0.2),\n",
    "        Dense(4, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(1, activation='sigmoid')  # Sigmoid for binary classification\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def create_model_Rnn_LSTM_GRU( modelType, max_words=max_words, embedding_dim=embedding_dim ):\n",
    "    print(f'Creating model type: {modelType}')\n",
    "    model = Sequential([\n",
    "        Dropout(0.2),\n",
    "        LSTM(64, return_sequences=True),\n",
    "        Dropout(0.2),\n",
    "        GRU(32),\n",
    "        Dropout(0.2),\n",
    "        Dense(4, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# CNN Models\n",
    "#   a) CNN + RNN + Transformer\n",
    "\n",
    "def create_model_CnnRnnTransformer(input_shape):\n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    # CNN part\n",
    "    cnn_layer = Conv1D(filters=128, kernel_size=5, activation='relu')(inputs)\n",
    "    cnn_layer = MaxPooling1D(pool_size=2)(cnn_layer)\n",
    "    cnn_layer = Conv1D(filters=64, kernel_size=5, activation='relu')(cnn_layer)\n",
    "    cnn_layer = MaxPooling1D(pool_size=2)(cnn_layer)\n",
    "\n",
    "    # RNN (LSTM or GRU)\n",
    "    rnn_layer = LSTM(128, return_sequences=True)(cnn_layer)\n",
    "    rnn_layer = Dropout(0.2)(rnn_layer)\n",
    "\n",
    "    # Optionally, use Bidirectional LSTM/GRU for better performance\n",
    "    rnn_layer = Bidirectional(LSTM(128,return_sequences=True))(rnn_layer)\n",
    "\n",
    "    # Transformer Layer using MultiHeadAttention\n",
    "    transformer_layer = tf.keras.layers.MultiHeadAttention(num_heads=4, key_dim=128)(rnn_layer, rnn_layer)\n",
    "    \n",
    "    # Pooling to reduce the sequence to a fixed size (optional global average pooling)\n",
    "    pooled_output = tf.keras.layers.GlobalAveragePooling1D()(transformer_layer)\n",
    "\n",
    "    # Fully Connected Layer\n",
    "    dense_layer = Dense(64, activation='relu')(pooled_output)\n",
    "    dense_layer = Dropout(0.5)(dense_layer)\n",
    "\n",
    "    # Output Layer (assuming binary classification)\n",
    "    outputs = Dense(1, activation='sigmoid')(dense_layer)\n",
    "\n",
    "    # Define the model\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: rnn_bidirectional_txt_sum.h5 loaded ...\n",
      "Epoch 1/20\n",
      "175/175 [==============================] - 5s 17ms/step - loss: 0.4548 - accuracy: 0.8578 - val_loss: 0.1509 - val_accuracy: 0.9481\n",
      "Epoch 2/20\n",
      "175/175 [==============================] - 2s 11ms/step - loss: 0.1523 - accuracy: 0.9499 - val_loss: 0.0955 - val_accuracy: 0.9684\n",
      "Epoch 3/20\n",
      "175/175 [==============================] - 2s 12ms/step - loss: 0.1030 - accuracy: 0.9673 - val_loss: 0.0720 - val_accuracy: 0.9770\n",
      "Epoch 4/20\n",
      "175/175 [==============================] - 2s 11ms/step - loss: 0.0782 - accuracy: 0.9774 - val_loss: 0.0630 - val_accuracy: 0.9806\n",
      "Epoch 5/20\n",
      "175/175 [==============================] - 2s 12ms/step - loss: 0.0643 - accuracy: 0.9832 - val_loss: 0.0679 - val_accuracy: 0.9817\n",
      "Epoch 6/20\n",
      "175/175 [==============================] - 2s 10ms/step - loss: 0.0571 - accuracy: 0.9846 - val_loss: 0.0659 - val_accuracy: 0.9820\n",
      "Epoch 7/20\n",
      "175/175 [==============================] - 2s 11ms/step - loss: 0.0553 - accuracy: 0.9856 - val_loss: 0.0617 - val_accuracy: 0.9831\n",
      "Epoch 8/20\n",
      "175/175 [==============================] - 2s 11ms/step - loss: 0.0508 - accuracy: 0.9868 - val_loss: 0.0626 - val_accuracy: 0.9838\n",
      "Epoch 9/20\n",
      "175/175 [==============================] - 2s 12ms/step - loss: 0.0479 - accuracy: 0.9874 - val_loss: 0.0633 - val_accuracy: 0.9840\n",
      "Epoch 10/20\n",
      "175/175 [==============================] - 2s 10ms/step - loss: 0.0486 - accuracy: 0.9865 - val_loss: 0.0656 - val_accuracy: 0.9840\n",
      "Epoch 11/20\n",
      "175/175 [==============================] - 2s 12ms/step - loss: 0.0400 - accuracy: 0.9900 - val_loss: 0.0681 - val_accuracy: 0.9838\n",
      "Epoch 12/20\n",
      "175/175 [==============================] - 2s 12ms/step - loss: 0.0406 - accuracy: 0.9892 - val_loss: 0.0708 - val_accuracy: 0.9838\n",
      "Epoch 13/20\n",
      "175/175 [==============================] - 2s 10ms/step - loss: 0.0406 - accuracy: 0.9894 - val_loss: 0.0724 - val_accuracy: 0.9835\n",
      "Epoch 14/20\n",
      "175/175 [==============================] - 2s 11ms/step - loss: 0.0425 - accuracy: 0.9885 - val_loss: 0.0731 - val_accuracy: 0.9835\n",
      "Epoch 15/20\n",
      "175/175 [==============================] - 2s 10ms/step - loss: 0.0382 - accuracy: 0.9898 - val_loss: 0.0716 - val_accuracy: 0.9837\n",
      "Epoch 16/20\n",
      "175/175 [==============================] - 2s 10ms/step - loss: 0.0381 - accuracy: 0.9901 - val_loss: 0.0717 - val_accuracy: 0.9833\n",
      "Epoch 17/20\n",
      "175/175 [==============================] - 2s 10ms/step - loss: 0.0348 - accuracy: 0.9910 - val_loss: 0.0728 - val_accuracy: 0.9829\n",
      "Epoch 18/20\n",
      "175/175 [==============================] - 2s 11ms/step - loss: 0.0358 - accuracy: 0.9907 - val_loss: 0.0736 - val_accuracy: 0.9837\n",
      "Epoch 19/20\n",
      "175/175 [==============================] - 2s 12ms/step - loss: 0.0378 - accuracy: 0.9897 - val_loss: 0.0739 - val_accuracy: 0.9840\n",
      "Epoch 20/20\n",
      "175/175 [==============================] - 2s 10ms/step - loss: 0.0369 - accuracy: 0.9899 - val_loss: 0.0746 - val_accuracy: 0.9826\n",
      "\n",
      "Training Time: 42.01 seconds\n",
      "\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.3453 - accuracy: 0.9207\n",
      "Test Accuracy: 0.9207\n",
      "469/469 [==============================] - 2s 3ms/step\n",
      "rnn_bidirectional_txt_sum.h5: Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.92      0.92      7490\n",
      "           1       0.92      0.92      0.92      7502\n",
      "\n",
      "    accuracy                           0.92     14992\n",
      "   macro avg       0.92      0.92      0.92     14992\n",
      "weighted avg       0.92      0.92      0.92     14992\n",
      "\n",
      "rnn_bidirectional_txt_sum.h5: Accuracy Score: 0.9207\n",
      "model: rnn_unidirectional_txt_sum.h5 loaded ...\n",
      "Epoch 1/20\n",
      "175/175 [==============================] - 3s 10ms/step - loss: 0.8669 - accuracy: 0.7105 - val_loss: 0.2060 - val_accuracy: 0.9316\n",
      "Epoch 2/20\n",
      "175/175 [==============================] - 1s 7ms/step - loss: 0.2035 - accuracy: 0.9318 - val_loss: 0.1364 - val_accuracy: 0.9644\n",
      "Epoch 3/20\n",
      "175/175 [==============================] - 1s 7ms/step - loss: 0.1380 - accuracy: 0.9611 - val_loss: 0.1091 - val_accuracy: 0.9741\n",
      "Epoch 4/20\n",
      "175/175 [==============================] - 1s 7ms/step - loss: 0.1080 - accuracy: 0.9747 - val_loss: 0.0946 - val_accuracy: 0.9813\n",
      "Epoch 5/20\n",
      "175/175 [==============================] - 1s 7ms/step - loss: 0.0930 - accuracy: 0.9798 - val_loss: 0.0906 - val_accuracy: 0.9817\n",
      "Epoch 6/20\n",
      "175/175 [==============================] - 1s 8ms/step - loss: 0.0818 - accuracy: 0.9849 - val_loss: 0.0857 - val_accuracy: 0.9824\n",
      "Epoch 7/20\n",
      "175/175 [==============================] - 1s 7ms/step - loss: 0.0734 - accuracy: 0.9871 - val_loss: 0.0847 - val_accuracy: 0.9813\n",
      "Epoch 8/20\n",
      "175/175 [==============================] - 2s 9ms/step - loss: 0.0704 - accuracy: 0.9868 - val_loss: 0.0844 - val_accuracy: 0.9824\n",
      "Epoch 9/20\n",
      "175/175 [==============================] - 1s 7ms/step - loss: 0.0622 - accuracy: 0.9889 - val_loss: 0.0840 - val_accuracy: 0.9828\n",
      "Epoch 10/20\n",
      "175/175 [==============================] - 1s 7ms/step - loss: 0.0604 - accuracy: 0.9887 - val_loss: 0.0789 - val_accuracy: 0.9842\n",
      "Epoch 11/20\n",
      "175/175 [==============================] - 2s 9ms/step - loss: 0.0555 - accuracy: 0.9904 - val_loss: 0.0870 - val_accuracy: 0.9835\n",
      "Epoch 12/20\n",
      "175/175 [==============================] - 1s 8ms/step - loss: 0.0532 - accuracy: 0.9906 - val_loss: 0.0911 - val_accuracy: 0.9824\n",
      "Epoch 13/20\n",
      "175/175 [==============================] - 1s 7ms/step - loss: 0.0501 - accuracy: 0.9915 - val_loss: 0.0801 - val_accuracy: 0.9840\n",
      "Epoch 14/20\n",
      "175/175 [==============================] - 1s 7ms/step - loss: 0.0474 - accuracy: 0.9916 - val_loss: 0.0841 - val_accuracy: 0.9829\n",
      "Epoch 15/20\n",
      "175/175 [==============================] - 1s 7ms/step - loss: 0.0488 - accuracy: 0.9909 - val_loss: 0.0870 - val_accuracy: 0.9824\n",
      "Epoch 16/20\n",
      "175/175 [==============================] - 1s 7ms/step - loss: 0.0441 - accuracy: 0.9909 - val_loss: 0.0824 - val_accuracy: 0.9838\n",
      "Epoch 17/20\n",
      "175/175 [==============================] - 1s 7ms/step - loss: 0.0442 - accuracy: 0.9913 - val_loss: 0.0829 - val_accuracy: 0.9828\n",
      "Epoch 18/20\n",
      "175/175 [==============================] - 1s 7ms/step - loss: 0.0401 - accuracy: 0.9920 - val_loss: 0.0783 - val_accuracy: 0.9829\n",
      "Epoch 19/20\n",
      "175/175 [==============================] - 1s 8ms/step - loss: 0.0388 - accuracy: 0.9925 - val_loss: 0.0874 - val_accuracy: 0.9842\n",
      "Epoch 20/20\n",
      "175/175 [==============================] - 1s 7ms/step - loss: 0.0373 - accuracy: 0.9930 - val_loss: 0.0756 - val_accuracy: 0.9838\n",
      "\n",
      "Training Time: 27.74 seconds\n",
      "\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.3428 - accuracy: 0.9173\n",
      "Test Accuracy: 0.9173\n",
      "469/469 [==============================] - 1s 2ms/step\n",
      "rnn_unidirectional_txt_sum.h5: Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.92      0.92      7490\n",
      "           1       0.92      0.91      0.92      7502\n",
      "\n",
      "    accuracy                           0.92     14992\n",
      "   macro avg       0.92      0.92      0.92     14992\n",
      "weighted avg       0.92      0.92      0.92     14992\n",
      "\n",
      "rnn_unidirectional_txt_sum.h5: Accuracy Score: 0.9173\n"
     ]
    }
   ],
   "source": [
    "############################################################################################\n",
    "#  Executing Neural Network Models - a) RNN + BiLSTM  + BiGRU                              #\n",
    "#                                    b) RNN + UniLSTM + UniGRU                             #\n",
    "#  epcochs   : 20                                                                          #\n",
    "#  batch_size: 128                                                                         #\n",
    "############################################################################################\n",
    "\n",
    "import os\n",
    "import joblib\n",
    "from keras.models import load_model\n",
    "\n",
    "models = { model_rnn_bidirectional_fname: create_model_Rnn_BiLSTM_BiGRU, model_rnn_unidirectional_fname: create_model_Rnn_LSTM_GRU }\n",
    "\n",
    "for model_name, model_function in models.items():\n",
    "    if( os.path.exists( model_name ) ):\n",
    "        model = load_model( model_name )\n",
    "        print(f'model: {model_name} loaded ...')\n",
    "    else:\n",
    "        model = model_function(\"RNN_BiLSTM_BiGRU\", max_words=max_words, embedding_dim=embedding_dim)\n",
    "\n",
    "    start_time = time.time()\n",
    "    history = model.fit(X_train_reshaped, y_train, \n",
    "                        epochs=nn_models_epochs, \n",
    "                        batch_size=nn_models_batchsize, \n",
    "                        validation_split=nn_models_validationSplit, \n",
    "                        verbose=1)\n",
    "    train_time = time.time() - start_time\n",
    "    print(f'\\nTraining Time: {train_time:.2f} seconds\\n')\n",
    "    start_time = time.time()\n",
    "    test_loss, test_accuracy = model.evaluate(X_test_reshaped, y_test)\n",
    "    \n",
    "    test_time = time.time() - start_time\n",
    "    print(f'Test Accuracy: {test_accuracy:.4f}')\n",
    "\n",
    "    y_pred = (model.predict(X_test_reshaped) > 0.50).astype(\"int32\")\n",
    "    print(f\"{model_name}: Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "    print(f\"{model_name}: Accuracy Score: {accuracy_score(y_test, y_pred):.4f}\")    \n",
    "    \n",
    "    report = classification_report(y_test,y_pred,output_dict=True)\n",
    "    precision = report['weighted avg']['precision']\n",
    "    recall    = report['weighted avg']['recall']\n",
    "    f1_score  = report['weighted avg']['f1-score']\n",
    "\n",
    "    model_results.append([model_name, f'{train_time:.2f} sec', f'{test_time:.2f} sec', f'{test_accuracy:.4f}',f'{precision:.4f}',f'{recall:.4f}',f'{f1_score:.4f}'])\n",
    "    model.save(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_name: randforest_txt_sum.h5\n",
      "model_name: adaboost_txt_sum.h5\n",
      "model_name: gradboost_txt_sum.h5\n",
      "#####  randforest_txt_sum.h5  #####\n",
      "model: randforest_txt_sum.h5 loaded ...\n",
      "regular training fit\n",
      "Training Time: 5.80 seconds\n",
      "\n",
      "[[6609  881]\n",
      " [1279 6223]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.88      0.86      7490\n",
      "           1       0.88      0.83      0.85      7502\n",
      "\n",
      "    accuracy                           0.86     14992\n",
      "   macro avg       0.86      0.86      0.86     14992\n",
      "weighted avg       0.86      0.86      0.86     14992\n",
      "\n",
      "#####  adaboost_txt_sum.h5  #####\n",
      "model: adaboost_txt_sum.h5 loaded ...\n",
      "regular training fit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fkao9\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Time: 28.17 seconds\n",
      "\n",
      "[[5614 1876]\n",
      " [1181 6321]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.75      0.79      7490\n",
      "           1       0.77      0.84      0.81      7502\n",
      "\n",
      "    accuracy                           0.80     14992\n",
      "   macro avg       0.80      0.80      0.80     14992\n",
      "weighted avg       0.80      0.80      0.80     14992\n",
      "\n",
      "#####  gradboost_txt_sum.h5  #####\n",
      "model: gradboost_txt_sum.h5 loaded ...\n",
      "regular training fit\n",
      "Training Time: 69.92 seconds\n",
      "\n",
      "[[6177 1313]\n",
      " [2354 5148]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.82      0.77      7490\n",
      "           1       0.80      0.69      0.74      7502\n",
      "\n",
      "    accuracy                           0.76     14992\n",
      "   macro avg       0.76      0.76      0.75     14992\n",
      "weighted avg       0.76      0.76      0.75     14992\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "models_cpu = {\n",
    "    model_randforest_fname : RandomForestClassifier,\n",
    "    model_adaboost_fname   : AdaBoostClassifier,\n",
    "#    model_bagging_fname    : BaggingClassifier,\n",
    "    model_gradboost_fname  : GradientBoostingClassifier\n",
    "}\n",
    "for model_name, model_ in models_cpu.items():\n",
    "    print(f'model_name: {model_name}')\n",
    "\n",
    "model_n_estimators=10\n",
    "for model_name, model_function in models_cpu.items():\n",
    "    print(f'#####  {model_name}  #####')\n",
    "\n",
    "    if( os.path.exists( model_name ) ):\n",
    "        model = joblib.load( model_name )\n",
    "        print(f'model: {model_name} loaded ...')\n",
    "    else:\n",
    "        model = model_function(n_estimators=model_n_estimators)\n",
    "    \n",
    "    # Check if the model supports partial fitting\n",
    "    start_time = time.time()\n",
    "    if hasattr(model, 'partial_fit'):\n",
    "        # For partial fit, we need to specify classes for classification models\n",
    "        print(f'partial training fit')\n",
    "        classes = np.unique(y_train)\n",
    "        model.partial_fit(X_train_dense, y_train, classes=classes)\n",
    "    else:\n",
    "        print(f'regular training fit')\n",
    "        model.fit(X_train_dense, y_train)      \n",
    "    \n",
    "    train_time = time.time() - start_time\n",
    "    print(f'Training Time: {train_time:.2f} seconds\\n')\n",
    "    start_time = time.time()\n",
    "    y_pred = model.predict(X_test_dense)\n",
    "    test_time = time.time() - start_time\n",
    "    \n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    print(f'{conf_matrix}')\n",
    "    print(f'{classification_report(y_test, y_pred)}')\n",
    "\n",
    "    test_accuracy = accuracy_score(y_test,y_pred)\n",
    "    \n",
    "    report = classification_report(y_test,y_pred,output_dict=True)\n",
    "    precision = report['weighted avg']['precision']\n",
    "    recall    = report['weighted avg']['recall']\n",
    "    f1_score  = report['weighted avg']['f1-score']\n",
    "\n",
    "    model_results.append([model_name, f'{train_time:.2f} sec', f'{test_time:.2f} sec', f'{test_accuracy:.4f}',f'{precision:.4f}',f'{recall:.4f}',f'{f1_score:.4f}'])\n",
    "   \n",
    "    joblib.dump( model, model_name )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fkao9\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [17:57:31] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"predictor\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Time: 23.29 seconds\n",
      "\n",
      "Confusion Matrix for XGBoost (GPU):\n",
      "[[6576  914]\n",
      " [1388 6114]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.88      0.85      7490\n",
      "           1       0.87      0.81      0.84      7502\n",
      "\n",
      "    accuracy                           0.85     14992\n",
      "   macro avg       0.85      0.85      0.85     14992\n",
      "weighted avg       0.85      0.85      0.85     14992\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['xgboost_txt_sum.h5']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##### XgBoost - CUDA #####\n",
    "import xgboost as xgb\n",
    "import os\n",
    "\n",
    "model_name = model_xgboost_fname\n",
    "\n",
    "dtrain = xgb.DMatrix(X_train_dense, label=y_train)\n",
    "dtest  = xgb.DMatrix(X_test_dense,  label=y_test )\n",
    "\n",
    "start_time = time.time()\n",
    "params = {\n",
    "    'objective': 'binary:logistic',  # Assuming binary classification\n",
    "    'tree_method': 'hist',           # Use GPU for training\n",
    "    'device'     : 'cuda',           # Use CUDA for GPU\n",
    "    'max_depth'  : 4,\n",
    "    'predictor'  : 'cpu_predictor'   # GPU for prediction as well\n",
    "}\n",
    "\n",
    "model = xgb.train(params, dtrain, num_boost_round=50)\n",
    "train_time = time.time() - start_time\n",
    "print(f'Training Time: {train_time:.2f} seconds\\n')\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "y_pred = model.predict(dtest)\n",
    "y_pred = [1 if pred > 0.5 else 0 for pred in y_pred]  # Convert probabilities to class labels\n",
    "\n",
    "test_time = time.time() - start_time\n",
    "\n",
    "# Confusion matrix and classification report\n",
    "print(\"Confusion Matrix for XGBoost (GPU):\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "test_accuracy = accuracy_score(y_test,y_pred)\n",
    "\n",
    "report = classification_report(y_test,y_pred,output_dict=True)\n",
    "precision = report['weighted avg']['precision']\n",
    "recall    = report['weighted avg']['recall']\n",
    "f1_score  = report['weighted avg']['f1-score']\n",
    "\n",
    "model_results.append([model_name, f'{train_time:.2f} sec', f'{test_time:.2f} sec', f'{test_accuracy:.4f}',f'{precision:.4f}',f'{recall:.4f}',f'{f1_score:.4f}'])\n",
    "\n",
    "joblib.dump( model,model_name )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ModelClassification_GridSearch(classifier, classifier_text, X_train,y_train,X_test,y_test,param_grid):\n",
    "    start_time = time.time()\n",
    "\n",
    "    grid_search = GridSearchCV(classifier, param_grid, cv=10, n_jobs=-1, scoring='accuracy',verbose=grid_search_verbose)\n",
    "    model_fit_param = grid_search.fit(X_train, y_train)\n",
    "    best_regressor  = grid_search.best_estimator_\n",
    "\n",
    "    train_time = time.time() - start_time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    y_pred                 = best_regressor.predict(X_test)\n",
    "    \n",
    "    test_time = time.time() - start_time\n",
    "    \n",
    "    model_accuracy         = accuracy_score(y_test,y_pred)\n",
    "    model_confusion_matrix = confusion_matrix(y_test,y_pred)\n",
    "    \n",
    "    report_classification = classification_report(y_test,y_pred, output_dict=True)\n",
    "\n",
    "    grid_training_score = best_regressor.score(X_train,y_train)\n",
    "    grid_testing_score  = best_regressor.score(X_test, y_test )\n",
    "\n",
    "    precision = report_classification['weighted avg']['precision']\n",
    "    recall    = report_classification['weighted avg']['recall']\n",
    "    f1_score  = report_classification['weighted avg']['f1-score']\n",
    "\n",
    "    model_results.append([classifier_text, f'{train_time:.2f} sec', f'{test_time:.2f} sec', f'{test_accuracy:.4f}',f'{precision:.4f}',f'{recall:.4f}',f'{f1_score:.4f}'])\n",
    "    \n",
    "    joblib.dump( best_regressor, classifier_text )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 9 candidates, totalling 90 fits\n",
      "Fitting 10 folds for each of 6 candidates, totalling 60 fits\n"
     ]
    }
   ],
   "source": [
    "if Proc_SVMGridSearch == 'TRUE':\n",
    "    param_grid_svc = {\n",
    "        'C': [0.1,1, 10], \n",
    "        'gamma': [1,0.1,0.01]\n",
    "    } \n",
    "    classifier_gridsearch_report = ModelClassification_GridSearch(SVC(), 'SVMGridSearch', X_train, y_train, X_test, y_test, param_grid_svc) \n",
    "\n",
    "if Proc_LogisticRegression == 'TRUE':\n",
    "    param_grid_lr = {\n",
    "        'C': [0.1, 1, 10],\n",
    "        'solver': ['liblinear', 'saga']\n",
    "    }\n",
    "    classifier_gridsearch_report = ModelClassification_GridSearch(LogisticRegression(), 'Logistic Regression GridSearch', X_train, y_train, X_test, y_test, param_grid_lr)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+---------------+--------------+----------+---------------------+------------------+--------------------+\n",
      "|             Model              | Training Time | Testing Time | Accuracy | Precision(weighted) | Recall(weighted) | f1-score(weighted) |\n",
      "+--------------------------------+---------------+--------------+----------+---------------------+------------------+--------------------+\n",
      "|  rnn_bidirectional_txt_sum.h5  |   42.01 sec   |   2.17 sec   |  0.9207  |       0.9207        |      0.9207      |       0.9207       |\n",
      "| rnn_unidirectional_txt_sum.h5  |   27.74 sec   |   1.50 sec   |  0.9173  |       0.9174        |      0.9173      |       0.9173       |\n",
      "|     randforest_txt_sum.h5      |   5.80 sec    |   0.21 sec   |  0.8559  |       0.8569        |      0.8559      |       0.8558       |\n",
      "|      adaboost_txt_sum.h5       |   28.17 sec   |   1.33 sec   |  0.7961  |       0.7986        |      0.7961      |       0.7956       |\n",
      "|      gradboost_txt_sum.h5      |   69.92 sec   |   0.19 sec   |  0.7554  |       0.7605        |      0.7554      |       0.7542       |\n",
      "|       xgboost_txt_sum.h5       |   23.29 sec   |   0.10 sec   |  0.8465  |       0.8478        |      0.8465      |       0.8463       |\n",
      "|         SVMGridSearch          |  4882.10 sec  |  64.38 sec   |  0.8465  |       0.9354        |      0.9350      |       0.9349       |\n",
      "| Logistic Regression GridSearch |   12.32 sec   |   0.01 sec   |  0.8465  |       0.9070        |      0.9006      |       0.9002       |\n",
      "+--------------------------------+---------------+--------------+----------+---------------------+------------------+--------------------+\n"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "print( tabulate(model_results, model_results_headers, tablefmt='pretty') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
